{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "(by Tevfik Aytekin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "def kaggle_score(y_true,y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred));\n",
    "#def kaggle_score(y_true,y_pred):\n",
    "#    return np.sqrt(mean_squared_error(np.log(y_true), np.log(y_pred)));\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_train = pd.read_csv(\"../../datasets/house_prices/train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor\n",
    "Run DecisionTreeRegressor on House Prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 25149.729071537287\n",
      "Test MAPE: 14.535382715783701\n",
      "Test Kaggle: 0.19832978682393862\n"
     ]
    }
   ],
   "source": [
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "mae, kaggle, mape = [], [], []\n",
    "for i in range(1,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "    regr = DecisionTreeRegressor()\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    test_predictions = regr.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test, test_predictions))\n",
    "    mape.append(mean_absolute_percentage_error(y_test, test_predictions))\n",
    "    kaggle.append(kaggle_score(y_test, test_predictions))\n",
    "\n",
    "print(\"Test MAE:\", np.mean(mae))\n",
    "print(\"Test MAPE:\", np.mean(mape))\n",
    "print(\"Test Kaggle:\", np.mean(kaggle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor\n",
    "Run sklearn's GradientBoostingRegressor on House Prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 15447.651721974144\n",
      "Test MAPE: 9.045724745406021\n",
      "Test Kaggle: 0.12663816676926054\n"
     ]
    }
   ],
   "source": [
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "mae, kaggle, mape = [], [], []\n",
    "for i in range(1,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "    regr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    test_predictions = regr.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test, test_predictions))\n",
    "    mape.append(mean_absolute_percentage_error(y_test, test_predictions))\n",
    "    kaggle.append(kaggle_score(y_test, test_predictions))\n",
    "\n",
    "print(\"Test MAE:\", np.mean(mae))\n",
    "print(\"Test MAPE:\", np.mean(mape))\n",
    "print(\"Test Kaggle:\", np.mean(kaggle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor from scratch\n",
    "Let us write GradientBoostingRegressor from scratch. Note that \"learning_rate\" is also known as \"shrinkage.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators = 100, learning_rate = 0.1, max_depth=3):\n",
    "        self.models = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "    def calc_grads(self, model, X, y):    \n",
    "        preds = self.learning_rate * model.predict(X)\n",
    "        grads = y - preds\n",
    "        return grads\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for m in self.models:\n",
    "            preds += self.learning_rate * m.predict(X)\n",
    "        return preds\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth);\n",
    "            if (i == 0):\n",
    "                model.fit(X, y)\n",
    "                grads = self.calc_grads(model, X, y)\n",
    "            else:\n",
    "                model.fit(X, grads)\n",
    "                grads = self.calc_grads(model, X, grads)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How learning_rate works\n",
    "\n",
    "learning_rate shrinks the contribution of each tree. This is an important parameter which has significant effect on the error. You can try to set it to 1 and look at the result. Below is a simple illustration:\n",
    "\n",
    "Suppose that we have only 3 models: M1, M2, and M3 <br>\n",
    "y = 10 <br>\n",
    "learning_rate = 1 (no shrinkage)\n",
    "\n",
    "\n",
    "M1.predict = 8 <br>\n",
    "grad = 10 - 8 = 2\n",
    "\n",
    "fit M2 for y = 2 <br>\n",
    "M2.predict = 1.5 <br>\n",
    "grad = 2 - 1.5 = 0.5\n",
    "\n",
    "fit M3 for y = 0.5\n",
    "\n",
    "\n",
    "Prediction:\n",
    "\n",
    "M1.predict(X) + M2.predict(X) + M3.predict(X)\n",
    "\n",
    "-----------------------\n",
    "3 models: M1, M2, and M3 <br>\n",
    "y = 10<br>\n",
    "learning_rate = 0.1 (with shrinkage)\n",
    "\n",
    "M1.predict = 8 <br>\n",
    "grad = 10 - 0.1 x 8 = 9.2\n",
    "\n",
    "fit M2 for y = 9.2<br>\n",
    "M2.predict = 7<br>\n",
    "grad = 9.2 - 0.1 * 7 = 8.5\n",
    "\n",
    "fit M3 for y = 8.5\n",
    "\n",
    "\n",
    "Prediction:\n",
    "\n",
    "0.1 x M1.predict(X) + 0.1 x M2.predict(X) + 0.1 x M3.predict(X) \n",
    "\n",
    "?? Should we multiply M3's prediction with 0.1?\n",
    "\n",
    "**Question:** Why does shrinkage improve performance significantly?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Run\n",
    "Now let us run our version of GradientBoostingRegressor on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.307269085250777\n",
      "11.149679196688858\n",
      "16.151335711659147\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "reg1 = DecisionTreeRegressor()\n",
    "reg2 = GradientBoostingRegressor()\n",
    "reg3 = MyGradientBoostingRegressor(learning_rate=1, max_depth=3)\n",
    "\n",
    "reg1.fit(X_train, y_train)\n",
    "reg2.fit(X_train, y_train)\n",
    "reg3.fit(X_train, y_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_test, reg1.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg2.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg3.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bank Marketing Dataset from\n",
    "# https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "\n",
    "bank = pd.read_csv(\"../datasets/bank/bank-full.csv\", delimiter = \";\")\n",
    "# print first 5 examples\n",
    "bank.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_majority = bank[bank.y==\"no\"]\n",
    "bank_minority = bank[bank.y==\"yes\"]\n",
    " \n",
    "# downsample\n",
    "bank_majority_downsampled = resample(bank_majority, \n",
    "                                 replace=False,    \n",
    "                                 n_samples=5289) \n",
    " \n",
    "bank_balanced = pd.concat([bank_minority, bank_majority_downsampled])\n",
    "bank_balanced.y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic/Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-6, 6, 0.1)\n",
    "s = 1 / (1 + np.exp(-x))\n",
    "plt.plot(x, s)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('s(x)')\n",
    "plt.suptitle(\"Logistic/Sigmoid function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "    \n",
    "    def cross_ent(self, y, p):\n",
    "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "    \n",
    "    def diff(self, y, p):\n",
    "        return y - p\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x)) \n",
    "    \n",
    "    def __init__(self, n_estimators = 100, learning_rate = 0.1, max_depth=3):\n",
    "        self.models = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def calc_grads(self, model, X, y):    \n",
    "        preds = self.learning_rate * model.predict(X)\n",
    "        grads = self.diff(y, preds)\n",
    "        return grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for m in self.models:\n",
    "            preds += self.learning_rate * m.predict(X)\n",
    "        return np.round(preds)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            model = DecisionTreeRegressor(max_depth=2);\n",
    "            if (i == 0):\n",
    "                model.fit(X, y)\n",
    "                grads = self.calc_grads(model, X, y)\n",
    "            else:\n",
    "                model.fit(X, grads)\n",
    "                grads = self.calc_grads(model, X, grads)\n",
    "            self.models.append(model)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    " \n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = GradientBoostingClassifier()\n",
    "clf3 = MyGradientBoostingClassifier()\n",
    "\n",
    "clf1.fit(X_train, y_train);\n",
    "clf2.fit(X_train, y_train);\n",
    "clf3.fit(X_train, y_train);\n",
    "\n",
    "y_pred1 = clf1.predict(X_test)  \n",
    "y_pred2 = clf2.predict(X_test) \n",
    "y_pred3 = clf3.predict(X_test) \n",
    "\n",
    "print(classification_report(y_test,y_pred1))\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Boosting Frameworks\n",
    "Boosting is the most popular/successful method for tabular datasets, hence there are many boosting libraries. Some of them are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.03 Elapsed time: 0.07 seconds\n",
      "9.55 Elapsed time: 1.11 seconds\n",
      "9.56 Elapsed time: 3.27 seconds\n",
      "9.88 Elapsed time: 0.83 seconds\n",
      "9.45 Elapsed time: 0.15 seconds\n",
      "8.46 Elapsed time: 3.71 seconds\n",
      "9.21 Elapsed time: 1.55 seconds\n",
      "10.22 Elapsed time: 3.68 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "\n",
    "models = [DecisionTreeRegressor(), GradientBoostingRegressor(), MyGradientBoostingRegressor(), \n",
    "          XGBRegressor(), LGBMRegressor(verbose= -1), CatBoostRegressor(verbose=False), \n",
    "          HistGradientBoostingRegressor(), RandomForestRegressor()]\n",
    "\n",
    "for model in models:\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time-start_time\n",
    "    mae = mean_absolute_percentage_error(y_test, model.predict(X_test))    \n",
    "    print(np.round(mae,2),f\"Elapsed time: {np.round(elapsed_time,2)} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries can handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.815124828097503\n",
      "10.320379532677274\n",
      "9.341487041963365\n",
      "9.742128691329663\n",
      "8.66274472557568\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "models = [DecisionTreeRegressor(), XGBRegressor(), LGBMRegressor(verbose= -1), \n",
    "          HistGradientBoostingRegressor(),CatBoostRegressor(verbose=False)]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    print(mean_absolute_percentage_error(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    " \n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = GradientBoostingClassifier()\n",
    "clf3 = MyGradientBoostingClassifier()\n",
    "clf4 = xgb.XGBClassifier()\n",
    "\n",
    "clf1.fit(X_train, y_train);\n",
    "clf2.fit(X_train, y_train);\n",
    "clf3.fit(X_train, y_train);\n",
    "clf4.fit(X_train, y_train);\n",
    "\n",
    "y_pred1 = clf1.predict(X_test)  \n",
    "y_pred2 = clf2.predict(X_test) \n",
    "y_pred3 = clf3.predict(X_test)\n",
    "y_pred4 = clf4.predict(X_test) \n",
    "\n",
    "print(classification_report(y_test,y_pred1))\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred3))\n",
    "print(classification_report(y_test,y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "clf4 = xgb.XGBClassifier()\n",
    "clf4.fit(X_train, y_train);\n",
    "y_pred4 = clf4.predict(X_test) \n",
    "print(classification_report(y_test,y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
