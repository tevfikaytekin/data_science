{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "(by Tevfik Aytekin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "def kaggle_score(y_true,y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred));\n",
    "#def kaggle_score(y_true,y_pred):\n",
    "#    return np.sqrt(mean_squared_error(np.log(y_true), np.log(y_pred)));\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_train = pd.read_csv(\"../../datasets/house_prices/train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor\n",
    "Run DecisionTreeRegressor on House Prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 25916.763318112633\n",
      "Test MAPE: 14.908966869852385\n",
      "Test Kaggle: 0.20654246544848162\n"
     ]
    }
   ],
   "source": [
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "mae, kaggle, mape = [], [], []\n",
    "for i in range(1,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "    regr = DecisionTreeRegressor()\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    test_predictions = regr.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test, test_predictions))\n",
    "    mape.append(mean_absolute_percentage_error(y_test, test_predictions))\n",
    "    kaggle.append(kaggle_score(y_test, test_predictions))\n",
    "\n",
    "print(\"Test MAE:\", np.mean(mae))\n",
    "print(\"Test MAPE:\", np.mean(mape))\n",
    "print(\"Test Kaggle:\", np.mean(kaggle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor\n",
    "Run sklearn's GradientBoostingRegressor on House Prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 16072.370398053528\n",
      "Test MAPE: 9.78273582613223\n",
      "Test Kaggle: 0.13962386572942842\n"
     ]
    }
   ],
   "source": [
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "mae, kaggle, mape = [], [], []\n",
    "for i in range(1,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "    regr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    test_predictions = regr.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test, test_predictions))\n",
    "    mape.append(mean_absolute_percentage_error(y_test, test_predictions))\n",
    "    kaggle.append(kaggle_score(y_test, test_predictions))\n",
    "\n",
    "print(\"Test MAE:\", np.mean(mae))\n",
    "print(\"Test MAPE:\", np.mean(mape))\n",
    "print(\"Test Kaggle:\", np.mean(kaggle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor from scratch\n",
    "Let us write GradientBoostingRegressor from scratch. Note that \"learning_rate\" is also known as \"shrinkage.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators = 100, learning_rate = 0.1, max_depth=3):\n",
    "        self.models = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "    def calc_grads(self, model, X, y):    \n",
    "        preds = self.learning_rate * model.predict(X)\n",
    "        grads = y - preds\n",
    "        return grads\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for m in self.models:\n",
    "            preds += self.learning_rate * m.predict(X)\n",
    "        return preds\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth);\n",
    "            if (i == 0):\n",
    "                model.fit(X, y)\n",
    "                grads = self.calc_grads(model, X, y)\n",
    "            else:\n",
    "                model.fit(X, grads)\n",
    "                grads = self.calc_grads(model, X, grads)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How learning_rate works\n",
    "\n",
    "learning_rate shrinks the contribution of each tree. This is an important parameter which has significant effect on the error. You can try to set it to 1 and look at the result. Below is a simple illustration:\n",
    "\n",
    "Suppose that we have only 3 models: M1, M2, and M3 <br>\n",
    "y = 10 <br>\n",
    "learning_rate = 1 (no shrinkage)\n",
    "\n",
    "\n",
    "M1.predict = 8 <br>\n",
    "grad = 10 - 8 = 2\n",
    "\n",
    "fit M2 for y = 2 <br>\n",
    "M2.predict = 1.5 <br>\n",
    "grad = 2 - 1.5 = 0.5\n",
    "\n",
    "fit M3 for y = 0.5\n",
    "\n",
    "\n",
    "Prediction:\n",
    "\n",
    "M1.predict(X) + M2.predict(X) + M3.predict(X)\n",
    "\n",
    "-----------------------\n",
    "3 models: M1, M2, and M3 <br>\n",
    "y = 10<br>\n",
    "learning_rate = 0.1 (with shrinkage)\n",
    "\n",
    "M1.predict = 8 <br>\n",
    "grad = 10 - 0.1 x 8 = 9.2\n",
    "\n",
    "fit M2 for y = 9.2<br>\n",
    "M2.predict = 7<br>\n",
    "grad = 9.2 - 0.1 * 7 = 8.5\n",
    "\n",
    "fit M3 for y = 8.5\n",
    "\n",
    "\n",
    "Prediction:\n",
    "\n",
    "0.1 x M1.predict(X) + 0.1 x M2.predict(X) + 0.1 x M3.predict(X) \n",
    "\n",
    "?? Should we multiply M3's prediction with 0.1?\n",
    "\n",
    "**Question:** Why does shrinkage improve performance significantly?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Run\n",
    "Now let us run our version of GradientBoostingRegressor on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.206124050726393\n",
      "8.162837981866712\n",
      "13.025230511408425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "reg1 = DecisionTreeRegressor()\n",
    "reg2 = GradientBoostingRegressor()\n",
    "reg3 = MyGradientBoostingRegressor(learning_rate=1, max_depth=3)\n",
    "\n",
    "reg1.fit(X_train, y_train)\n",
    "reg2.fit(X_train, y_train)\n",
    "reg3.fit(X_train, y_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_test, reg1.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg2.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg3.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bank Marketing Dataset from\n",
    "# https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "\n",
    "bank = pd.read_csv(\"../../datasets/bank/bank-full.csv\", delimiter = \";\")\n",
    "# print first 5 examples\n",
    "bank.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "no     39922\n",
       "yes     5289\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "yes    5289\n",
       "no     5289\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_majority = bank[bank.y==\"no\"]\n",
    "bank_minority = bank[bank.y==\"yes\"]\n",
    " \n",
    "# downsample\n",
    "bank_majority_downsampled = resample(bank_majority, \n",
    "                                 replace=False,    \n",
    "                                 n_samples=5289) \n",
    " \n",
    "bank_balanced = pd.concat([bank_minority, bank_majority_downsampled])\n",
    "bank_balanced.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "    \n",
    "    def cross_ent(self, y, p):\n",
    "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "    \n",
    "    def diff(self, y, p):\n",
    "        return y - p\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x)) \n",
    "    \n",
    "    def __init__(self, n_estimators = 100, learning_rate = 0.1, max_depth=3):\n",
    "        self.models = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def calc_grads(self, model, X, y):    \n",
    "        preds = self.learning_rate * model.predict(X)\n",
    "        grads = self.diff(y, preds)\n",
    "        return grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for m in self.models:\n",
    "            preds += self.learning_rate * m.predict(X)\n",
    "        return np.round(preds)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            model = DecisionTreeRegressor(max_depth=2);\n",
    "            if (i == 0):\n",
    "                model.fit(X, y)\n",
    "                grads = self.calc_grads(model, X, y)\n",
    "            else:\n",
    "                model.fit(X, grads)\n",
    "                grads = self.calc_grads(model, X, grads)\n",
    "            self.models.append(model)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79       517\n",
      "           1       0.81      0.76      0.78       541\n",
      "\n",
      "    accuracy                           0.79      1058\n",
      "   macro avg       0.79      0.79      0.79      1058\n",
      "weighted avg       0.79      0.79      0.79      1058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87       517\n",
      "           1       0.86      0.90      0.88       541\n",
      "\n",
      "    accuracy                           0.87      1058\n",
      "   macro avg       0.88      0.87      0.87      1058\n",
      "weighted avg       0.87      0.87      0.87      1058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85       517\n",
      "           1       0.85      0.87      0.86       541\n",
      "\n",
      "    accuracy                           0.86      1058\n",
      "   macro avg       0.86      0.86      0.86      1058\n",
      "weighted avg       0.86      0.86      0.86      1058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    " \n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = GradientBoostingClassifier()\n",
    "clf3 = MyGradientBoostingClassifier()\n",
    "\n",
    "clf1.fit(X_train, y_train);\n",
    "clf2.fit(X_train, y_train);\n",
    "clf3.fit(X_train, y_train);\n",
    "\n",
    "y_pred1 = clf1.predict(X_test)  \n",
    "y_pred2 = clf2.predict(X_test) \n",
    "y_pred3 = clf3.predict(X_test) \n",
    "\n",
    "print(classification_report(y_test,y_pred1))\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Boosting Frameworks\n",
    "Boosting is the most popular/successful method for tabular datasets, hence there are many boosting libraries. Some of them are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.03 Elapsed time: 0.07 seconds\n",
      "9.55 Elapsed time: 1.11 seconds\n",
      "9.56 Elapsed time: 3.27 seconds\n",
      "9.88 Elapsed time: 0.83 seconds\n",
      "9.45 Elapsed time: 0.15 seconds\n",
      "8.46 Elapsed time: 3.71 seconds\n",
      "9.21 Elapsed time: 1.55 seconds\n",
      "10.22 Elapsed time: 3.68 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "\n",
    "models = [DecisionTreeRegressor(), GradientBoostingRegressor(), MyGradientBoostingRegressor(), \n",
    "          XGBRegressor(), LGBMRegressor(verbose= -1), CatBoostRegressor(verbose=False), \n",
    "          HistGradientBoostingRegressor(), RandomForestRegressor()]\n",
    "\n",
    "for model in models:\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time-start_time\n",
    "    mae = mean_absolute_percentage_error(y_test, model.predict(X_test))    \n",
    "    print(np.round(mae,2),f\"Elapsed time: {np.round(elapsed_time,2)} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79 Elapsed time: 0.11 seconds\n",
      "0.85 Elapsed time: 1.68 seconds\n",
      "0.84 Elapsed time: 1.7 seconds\n",
      "0.86 Elapsed time: 0.84 seconds\n",
      "0.87 Elapsed time: 0.12 seconds\n",
      "0.87 Elapsed time: 3.22 seconds\n",
      "0.86 Elapsed time: 0.46 seconds\n",
      "0.85 Elapsed time: 1.17 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "\n",
    "models = [DecisionTreeClassifier(), GradientBoostingClassifier(), MyGradientBoostingClassifier(), \n",
    "          XGBClassifier(), LGBMClassifier(verbose= -1), CatBoostClassifier(verbose=False), \n",
    "          HistGradientBoostingClassifier(), RandomForestClassifier()]\n",
    "\n",
    "for model in models:\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time-start_time\n",
    "    acc = accuracy_score(y_test, model.predict(X_test))    \n",
    "    print(np.round(acc,2),f\"Elapsed time: {np.round(elapsed_time,2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries can handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.815124828097503\n",
      "10.320379532677274\n",
      "9.341487041963365\n",
      "9.742128691329663\n",
      "8.66274472557568\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "models = [DecisionTreeRegressor(), XGBRegressor(), LGBMRegressor(verbose= -1), \n",
    "          HistGradientBoostingRegressor(),CatBoostRegressor(verbose=False)]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    print(mean_absolute_percentage_error(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    " \n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = GradientBoostingClassifier()\n",
    "clf3 = MyGradientBoostingClassifier()\n",
    "clf4 = xgb.XGBClassifier()\n",
    "\n",
    "clf1.fit(X_train, y_train);\n",
    "clf2.fit(X_train, y_train);\n",
    "clf3.fit(X_train, y_train);\n",
    "clf4.fit(X_train, y_train);\n",
    "\n",
    "y_pred1 = clf1.predict(X_test)  \n",
    "y_pred2 = clf2.predict(X_test) \n",
    "y_pred3 = clf3.predict(X_test)\n",
    "y_pred4 = clf4.predict(X_test) \n",
    "\n",
    "print(classification_report(y_test,y_pred1))\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred3))\n",
    "print(classification_report(y_test,y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "clf4 = xgb.XGBClassifier()\n",
    "clf4.fit(X_train, y_train);\n",
    "y_pred4 = clf4.predict(X_test) \n",
    "print(classification_report(y_test,y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
