{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+H6d4MSXJ5Nnnp7c3LfIZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tEw5d1K2PTmh"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","import torch.optim as optim\n","from collections import Counter\n","from itertools import chain\n"]},{"cell_type":"markdown","source":["### Load the dataset\n"],"metadata":{"id":"41g18QDs8vhP"}},{"cell_type":"code","source":["df = pd.read_csv('bbc-text.csv')  # Replace with your dataset path\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUOn2zh2XaE1","executionInfo":{"status":"ok","timestamp":1732644060554,"user_tz":-180,"elapsed":266,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"13c73e95-ebc8-4dbb-9075-e85483935f44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["        category                                               text\n","0           tech  tv future in the hands of viewers with home th...\n","1       business  worldcom boss  left books alone  former worldc...\n","2          sport  tigers wary of farrell  gamble  leicester say ...\n","3          sport  yeading face newcastle in fa cup premiership s...\n","4  entertainment  ocean s twelve raids box office ocean s twelve...\n"]}]},{"cell_type":"markdown","source":["### Preprocess the data\n"],"metadata":{"id":"0Brkdaw08y6w"}},{"cell_type":"code","source":["label_encoder = LabelEncoder()\n","df['category_encoded'] = label_encoder.fit_transform(df['category'])\n","\n","# Split into training and test sets\n","train_df, test_df = train_test_split(df, test_size=0.1)\n","\n","# Tokenizer function\n","def basic_english_tokenizer(text):\n","    return text.lower().split()  # Basic whitespace and lowercase tokenizer\n","\n","# Build vocabulary manually\n","def build_vocab(data_iter, tokenizer, specials=[\"<unk>\"]):\n","    counter = Counter(chain.from_iterable(tokenizer(text) for text in data_iter))\n","    sorted_vocab = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n","    vocab = {word: idx + len(specials) for idx, (word, _) in enumerate(sorted_vocab)}\n","    for idx, special in enumerate(specials):\n","        vocab[special] = idx\n","    return vocab\n","\n","# Yield tokens\n","train_texts = train_df['text'].tolist()\n","vocab = build_vocab(train_texts, basic_english_tokenizer)\n","vocab[\"<unk>\"] = 0  # Set <unk> as the default index\n","print(f\"Vocabulary size: {len(vocab)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5XwKJ36_YQGr","executionInfo":{"status":"ok","timestamp":1732644643913,"user_tz":-180,"elapsed":456,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"07392463-f9bc-4459-87be-a95d1dd5088c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 41741\n"]}]},{"cell_type":"markdown","source":["The `build_vocab` function returns a dictionary (`dict`) where:\n","- **Keys**: Unique tokens (words) from the training dataset.\n","- **Values**: Integer indices assigned to each token, starting with the indices for special tokens.\n"],"metadata":{"id":"BvJDaKI9h76k"}},{"cell_type":"code","source":["list(vocab.items())[0:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ylYXKTt8i2mU","executionInfo":{"status":"ok","timestamp":1732644723115,"user_tz":-180,"elapsed":500,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"0b31e82b-171d-48e9-d0b8-45e549ff639a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('the', 1), ('to', 2), ('of', 3), ('and', 4), ('a', 5)]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["### Custom Dataset Class\n"],"metadata":{"id":"OQVRJCeL83HV"}},{"cell_type":"code","source":["class BBCDataset(Dataset):\n","    def __init__(self, dataframe, vocab, tokenizer, max_length=500):\n","        self.dataframe = dataframe\n","        self.vocab = vocab\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        text = self.dataframe.iloc[idx]['text']\n","        label = self.dataframe.iloc[idx]['category_encoded']\n","        tokens = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in self.tokenizer(text)]\n","        if len(tokens) < self.max_length:\n","            tokens += [0] * (self.max_length - len(tokens))  # Padding\n","        else:\n","            tokens = tokens[:self.max_length]  # Truncating\n","        return torch.tensor(tokens, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n","\n","# Prepare Dataloaders\n","train_dataset = BBCDataset(train_df, vocab, basic_english_tokenizer)\n","test_dataset = BBCDataset(test_df, vocab, basic_english_tokenizer)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Print vocabulary size\n","print(f\"Vocabulary size: {len(vocab)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7k1B1KWZwE8","executionInfo":{"status":"ok","timestamp":1732644932235,"user_tz":-180,"elapsed":302,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"ce289a48-fb8a-41e3-c332-94e0dab0c16d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 41741\n"]}]},{"cell_type":"markdown","source":["### Summary of `__getitem__` Method in `BBCDataset`\n","\n","The `__getitem__` method of the `BBCDataset` class returns a **tuple** containing:\n","1. **Tokenized and Padded/Truncated Text**:\n","   - A PyTorch tensor representing the numerical sequence of tokens for the text at a given index.\n","   - Tokens are mapped to their vocabulary indices.\n","   - The sequence is padded with `0`s or truncated to a fixed length (`max_length`).\n","\n","2. **Encoded Label**:\n","   - A PyTorch tensor representing the encoded category label of the text.\n","\n","\n"],"metadata":{"id":"IJm1xvxBps0P"}},{"cell_type":"code","source":["train_dataset[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4DLtGlxApyg1","executionInfo":{"status":"ok","timestamp":1732644986485,"user_tz":-180,"elapsed":373,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"5a3d2d5e-28c4-47da-d07e-ce3c1cbdeadc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([  433,   867,  6943, 16869,   374,   356,   338,   252,    19,   648,\n","           454,   149,     3,  3236, 16869,    64,   631,    12,   271,   804,\n","            81, 13751,     1,    75,    19,  1116,    57, 16870,     1,  1278,\n","           310,    36,  2098,  3466,   804,     6,  7555,  7556,    33,   252,\n","          2960,     1,   318,  4595,  2510,    11,     9,    28,  8240,    45,\n","            97,    15, 11677,    11,     7,     1, 13751,   138,   371,   589,\n","          1200,    17,   618,    38,  1104,     2,    40,  3582,     2,  1384,\n","             1,  2057,  5039,   391,  1878,   541,   372,  4052,    65,   359,\n","          1228,     5,   374,   990,     2,   416,     1,   310,    79,    83,\n","          8240,   271, 10210,  9081,   804,    61,    15, 16871,     1,   707,\n","          2296,     1,   262,   159,     2,   148,     1,   391,     4,  2724,\n","          2868,   203,    37,    15,  7557,    18,   143,     3,     5,  1384,\n","             3,     1,  2057,    14,    91, 11678,    59,    31,    89,     1,\n","           216,   881,    27,   128,    47,    31,    23,     3,   821,   364,\n","             5,   416,     6,     1,  2057,   624,     1,    75,    19,   868,\n","            85,   416,     9,   490,    24,    50,    23,    15,     5,  5040,\n","           349,    79,    47,    25,   458,    30,    25,  3152,     2,  2190,\n","          4223,    12,   825,    33,   252,    91,   114,   260,   758,     7,\n","           546,   685,    33,   149,    38,  1748,  1385,     1,   310,   416,\n","           124,    38,   192,  1061,     2,     1,   739,     4,    13,    77,\n","         23516,  1784,    14,   120,    31,   274,    38,   110,   605,  3715,\n","             6,   110,   233,     4,    71, 23517,    11,     7,    28,  3892,\n","           708,   200,    91, 16872,    50,     7,   677,    62,    61,   148,\n","             2,   206,    77,    10,    26,    61,   483, 10211,    33,   252,\n","            17,     1, 10210,  9081,   713,  4830,     1, 16873,   618,    38,\n","          1229,     8,   898,  1171,   164, 23518,   164,   728,  2511,    27,\n","         16874,     1,   476,  2655,     1,   163,  6422,    66,  6423,  9082,\n","           454,   742,    13,  4419,     8,  3368,  1785,     5, 11679,  6943,\n","             6,  2099,    18,    14,  1817,   350,    27,     1, 13752,    24,\n","             1,   138,   371,    17,    33,   252,    13,   954,     2,   148,\n","             1,   310,    13, 11680,     6,  1602,     3, 13751,     1, 11681,\n","            45,    38,  3467,    33,   742,    38,    17,     1,   631,    43,\n","         16875,  9083,     6,  1602,     3,     1, 16876,    17,    33, 11682,\n","            14,   120,     1,   547,     9,    50,    19,    28,    39,   451,\n","          2656,     3,    11,    29,    10,    13,     1,   472,   541,   372,\n","          4052,    13,   297,     4,     1,   262,   159,    13, 11683,     1,\n","            46,  4224,    37,   205,   718,  9084,    12,     1,   476,   343,\n","            33, 11682,     1,   207,     3,   163,  7558,  2869,  8241,   137,\n","            83,   686,  8240,    55, 23519,     3,   246,  5323,    20,    39,\n","          6944,     6,     1,    65,  1080,   434,   128,  1171,   790,     5,\n","         11684,   491,    45,    38,  8242,     8,     5,  6943,  7559,   152,\n","            48,   492,   152,     4,   119,   152, 23520,    14,    58,   374,\n","           853,  3893, 13753,     7,   800,   942,     7,   442,     2,   416,\n","             1,   310,   632,     5,    67,  2009,     6,   909,    12,   638,\n","             4,  1149,     2,     5,   521,  1053,   103,   991,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n"," tensor(2))"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["These are then used by a `DataLoader` to create batches for training or evaluation."],"metadata":{"id":"RaQ7AT01rR1f"}},{"cell_type":"code","source":["for texts, labels in train_loader:\n","    print(texts.shape)\n","    print(labels.shape)\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPP9oan2b24_","executionInfo":{"status":"ok","timestamp":1732645034408,"user_tz":-180,"elapsed":391,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"718874f7-f27f-4ce0-b2e4-199147e6ba89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 500])\n","torch.Size([32])\n"]}]},{"cell_type":"code","source":["texts"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHB0s1COh1GZ","executionInfo":{"status":"ok","timestamp":1732645082943,"user_tz":-180,"elapsed":412,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"b5b83e00-535f-41e4-cf1e-ef8927ac9929"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 5271,   736,     8,  ...,     0,     0,     0],\n","        [ 5609,   853,     7,  ...,     0,     0,     0],\n","        [ 4984,  4286,  2714,  ...,    41,  1530,  7345],\n","        ...,\n","        [  200,  6303,  6894,  ...,   262,   159,   454],\n","        [ 7451,  1153,   142,  ...,     0,     0,     0],\n","        [12244,  1900,   463,  ...,     0,     0,     0]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["### Define the Model\n"],"metadata":{"id":"AxwXb7cj87bN"}},{"cell_type":"code","source":["class TextClassificationModel(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, num_classes):\n","        super(TextClassificationModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.fc = nn.Linear(embed_dim, num_classes)\n","        self.pool = nn.AdaptiveAvgPool1d(1)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)  # (batch_size, seq_length, embed_dim)\n","        embedded = embedded.permute(0, 2, 1)  # (batch_size, embed_dim, seq_length)\n","        pooled = self.pool(embedded).squeeze(2)  # (batch_size, embed_dim)\n","        output = self.fc(pooled)  # (batch_size, num_classes)\n","        return output"],"metadata":{"id":"cJPKTVOIYIiL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = len(vocab)\n","embed_dim = 100\n","num_classes = len(label_encoder.classes_)\n","embedding = nn.Embedding(vocab_size, embed_dim)"],"metadata":{"id":"qLhQ_1BBrkYX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedded = embedding(texts)\n","embedded.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-i865BjzrqGp","executionInfo":{"status":"ok","timestamp":1732645289997,"user_tz":-180,"elapsed":343,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"d8d50083-eacb-4c49-9e91-a5fc32f457d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 500, 100])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["embedded = embedded.permute(0,2,1)\n","embedded.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"he34OB0YrqCg","executionInfo":{"status":"ok","timestamp":1732645331978,"user_tz":-180,"elapsed":391,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"8bdfdbea-9caa-4ca3-ca99-c18cbd23b6fe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 100, 500])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["pool = nn.AdaptiveAvgPool1d(1)\n","pooled = pool(embedded).squeeze(2)\n","pooled.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xgtn6U_Mrp_o","executionInfo":{"status":"ok","timestamp":1732645337507,"user_tz":-180,"elapsed":263,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"570becd5-1996-4dbb-a311-3764e287b065"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 100])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["fc = nn.Linear(embed_dim, num_classes)\n","output = fc(pooled)\n","output.shape\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RMEZlwcqrp8x","executionInfo":{"status":"ok","timestamp":1732645392395,"user_tz":-180,"elapsed":404,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"1859c5cc-3d65-4e3a-a01f-51f3d3ae81d1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 5])"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["### Train model"],"metadata":{"id":"0JSnQNCZ8-u3"}},{"cell_type":"code","source":["# Model Parameters\n","vocab_size = len(vocab)\n","embed_dim = 100\n","num_classes = len(label_encoder.classes_)\n","\n","model = TextClassificationModel(vocab_size, embed_dim, num_classes)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training Loop\n","epochs = 20\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","    for texts, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(texts)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LsMI6k4cYxjc","executionInfo":{"status":"ok","timestamp":1732645541670,"user_tz":-180,"elapsed":93831,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"c63b8887-c460-43a5-9513-0a2ef6b2e2e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Loss: 1.5828\n","Epoch 2/20, Loss: 1.5030\n","Epoch 3/20, Loss: 1.4481\n","Epoch 4/20, Loss: 1.3828\n","Epoch 5/20, Loss: 1.3070\n","Epoch 6/20, Loss: 1.2106\n","Epoch 7/20, Loss: 1.1003\n","Epoch 8/20, Loss: 0.9777\n","Epoch 9/20, Loss: 0.8540\n","Epoch 10/20, Loss: 0.7385\n","Epoch 11/20, Loss: 0.6347\n","Epoch 12/20, Loss: 0.5460\n","Epoch 13/20, Loss: 0.4664\n","Epoch 14/20, Loss: 0.4013\n","Epoch 15/20, Loss: 0.3460\n","Epoch 16/20, Loss: 0.3016\n","Epoch 17/20, Loss: 0.2647\n","Epoch 18/20, Loss: 0.2313\n","Epoch 19/20, Loss: 0.2051\n","Epoch 20/20, Loss: 0.1823\n"]}]},{"cell_type":"markdown","source":["### Evaluation\n"],"metadata":{"id":"jDNQBPAA9Bxa"}},{"cell_type":"code","source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for texts, labels in test_loader:\n","        outputs = model(texts)\n","        outputs = model(texts)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = correct / total\n","print(f'Test Accuracy: {accuracy:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OIyGpMZyYm6q","executionInfo":{"status":"ok","timestamp":1732645552553,"user_tz":-180,"elapsed":466,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"6fd72027-7f36-4139-9226-79f67622e85b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.9552\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2NKRFxBHw9ko"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Adaptive Pooling Example"],"metadata":{"id":"SmbNh2Hq8pFh"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Example input tensor of shape (batch_size=1, channels=1, length=8)\n","input_tensor = torch.tensor([[[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]]])\n","print(\"Input tensor:\")\n","print(input_tensor)\n","\n","# Create an AdaptiveAvgPool1d layer that outputs a size of 4\n","adaptive_avg_pool = nn.AdaptiveAvgPool1d(1)\n","\n","# Apply the layer\n","output_tensor = adaptive_avg_pool(input_tensor)\n","\n","print(\"\\nOutput tensor after applying AdaptiveAvgPool1d:\")\n","print(output_tensor)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhxb_RHpw9Vv","executionInfo":{"status":"ok","timestamp":1731689974418,"user_tz":-180,"elapsed":5,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"bca94dda-3eca-406e-ead7-b4049d0a9a86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input tensor:\n","tensor([[[1., 2., 3., 4., 5., 6., 7., 8.]]])\n","\n","Output tensor after applying AdaptiveAvgPool1d:\n","tensor([[[4.5000]]])\n"]}]},{"cell_type":"code","source":["output_tensor.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iuW-y6wYxAhp","executionInfo":{"status":"ok","timestamp":1731689993110,"user_tz":-180,"elapsed":4,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"8119074d-8dc2-44d4-ba3e-b148c1be7899"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 1, 1])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["output_tensor.squeeze(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6OwP9AUoxFFI","executionInfo":{"status":"ok","timestamp":1731690009243,"user_tz":-180,"elapsed":317,"user":{"displayName":"Tevfik Aytekin","userId":"03705756795675396046"}},"outputId":"8341fe13-936b-4497-fe94-023eb49ec5c0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[4.5000]])"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":[],"metadata":{"id":"PEzYo-KWxI-w"},"execution_count":null,"outputs":[]}]}