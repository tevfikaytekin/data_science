{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEw5d1K2PTmh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41g18QDs8vhP"
   },
   "source": [
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1732644060554,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "QUOn2zh2XaE1",
    "outputId": "13c73e95-ebc8-4dbb-9075-e85483935f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        category                                               text\n",
      "0           tech  tv future in the hands of viewers with home th...\n",
      "1       business  worldcom boss  left books alone  former worldc...\n",
      "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
      "3          sport  yeading face newcastle in fa cup premiership s...\n",
      "4  entertainment  ocean s twelve raids box office ocean s twelve...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bbc-text.csv')  # Replace with your dataset path\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Brkdaw08y6w"
   },
   "source": [
    "### Preprocess the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1732644643913,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "5XwKJ36_YQGr",
    "outputId": "07392463-f9bc-4459-87be-a95d1dd5088c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 41741\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['category_encoded'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# Split into training and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "\n",
    "# Tokenizer function\n",
    "def basic_english_tokenizer(text):\n",
    "    return text.lower().split()  # Basic whitespace and lowercase tokenizer\n",
    "\n",
    "# Build vocabulary manually\n",
    "def build_vocab(data_iter, tokenizer, specials=[\"<unk>\"]):\n",
    "    counter = Counter(chain.from_iterable(tokenizer(text) for text in data_iter))\n",
    "    sorted_vocab = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    vocab = {word: idx + len(specials) for idx, (word, _) in enumerate(sorted_vocab)}\n",
    "    for idx, special in enumerate(specials):\n",
    "        vocab[special] = idx\n",
    "    return vocab\n",
    "\n",
    "# Yield tokens\n",
    "train_texts = train_df['text'].tolist()\n",
    "vocab = build_vocab(train_texts, basic_english_tokenizer)\n",
    "vocab[\"<unk>\"] = 0  # Set <unk> as the default index\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvJDaKI9h76k"
   },
   "source": [
    "The `build_vocab` function returns a dictionary (`dict`) where:\n",
    "- **Keys**: Unique tokens (words) from the training dataset.\n",
    "- **Values**: Integer indices assigned to each token, starting with the indices for special tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1732644723115,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "ylYXKTt8i2mU",
    "outputId": "0b31e82b-171d-48e9-d0b8-45e549ff639a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1), ('to', 2), ('of', 3), ('and', 4), ('a', 5)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.items())[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQVRJCeL83HV"
   },
   "source": [
    "### Custom Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1732644932235,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "c7k1B1KWZwE8",
    "outputId": "ce289a48-fb8a-41e3-c332-94e0dab0c16d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 41741\n"
     ]
    }
   ],
   "source": [
    "class BBCDataset(Dataset):\n",
    "    def __init__(self, dataframe, vocab, tokenizer, max_length=500):\n",
    "        self.dataframe = dataframe\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataframe.iloc[idx]['text']\n",
    "        label = self.dataframe.iloc[idx]['category_encoded']\n",
    "        tokens = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in self.tokenizer(text)]\n",
    "        if len(tokens) < self.max_length:\n",
    "            tokens += [0] * (self.max_length - len(tokens))  # Padding\n",
    "        else:\n",
    "            tokens = tokens[:self.max_length]  # Truncating\n",
    "        return torch.tensor(tokens, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Prepare Dataloaders\n",
    "train_dataset = BBCDataset(train_df, vocab, basic_english_tokenizer)\n",
    "test_dataset = BBCDataset(test_df, vocab, basic_english_tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print vocabulary size\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJm1xvxBps0P"
   },
   "source": [
    "### Summary of `__getitem__` Method in `BBCDataset`\n",
    "\n",
    "The `__getitem__` method of the `BBCDataset` class returns a **tuple** containing:\n",
    "1. **Tokenized and Padded/Truncated Text**:\n",
    "   - A PyTorch tensor representing the numerical sequence of tokens for the text at a given index.\n",
    "   - Tokens are mapped to their vocabulary indices.\n",
    "   - The sequence is padded with `0`s or truncated to a fixed length (`max_length`).\n",
    "\n",
    "2. **Encoded Label**:\n",
    "   - A PyTorch tensor representing the encoded category label of the text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1732644986485,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "4DLtGlxApyg1",
    "outputId": "5a3d2d5e-28c4-47da-d07e-ce3c1cbdeadc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  433,   867,  6943, 16869,   374,   356,   338,   252,    19,   648,\n",
       "           454,   149,     3,  3236, 16869,    64,   631,    12,   271,   804,\n",
       "            81, 13751,     1,    75,    19,  1116,    57, 16870,     1,  1278,\n",
       "           310,    36,  2098,  3466,   804,     6,  7555,  7556,    33,   252,\n",
       "          2960,     1,   318,  4595,  2510,    11,     9,    28,  8240,    45,\n",
       "            97,    15, 11677,    11,     7,     1, 13751,   138,   371,   589,\n",
       "          1200,    17,   618,    38,  1104,     2,    40,  3582,     2,  1384,\n",
       "             1,  2057,  5039,   391,  1878,   541,   372,  4052,    65,   359,\n",
       "          1228,     5,   374,   990,     2,   416,     1,   310,    79,    83,\n",
       "          8240,   271, 10210,  9081,   804,    61,    15, 16871,     1,   707,\n",
       "          2296,     1,   262,   159,     2,   148,     1,   391,     4,  2724,\n",
       "          2868,   203,    37,    15,  7557,    18,   143,     3,     5,  1384,\n",
       "             3,     1,  2057,    14,    91, 11678,    59,    31,    89,     1,\n",
       "           216,   881,    27,   128,    47,    31,    23,     3,   821,   364,\n",
       "             5,   416,     6,     1,  2057,   624,     1,    75,    19,   868,\n",
       "            85,   416,     9,   490,    24,    50,    23,    15,     5,  5040,\n",
       "           349,    79,    47,    25,   458,    30,    25,  3152,     2,  2190,\n",
       "          4223,    12,   825,    33,   252,    91,   114,   260,   758,     7,\n",
       "           546,   685,    33,   149,    38,  1748,  1385,     1,   310,   416,\n",
       "           124,    38,   192,  1061,     2,     1,   739,     4,    13,    77,\n",
       "         23516,  1784,    14,   120,    31,   274,    38,   110,   605,  3715,\n",
       "             6,   110,   233,     4,    71, 23517,    11,     7,    28,  3892,\n",
       "           708,   200,    91, 16872,    50,     7,   677,    62,    61,   148,\n",
       "             2,   206,    77,    10,    26,    61,   483, 10211,    33,   252,\n",
       "            17,     1, 10210,  9081,   713,  4830,     1, 16873,   618,    38,\n",
       "          1229,     8,   898,  1171,   164, 23518,   164,   728,  2511,    27,\n",
       "         16874,     1,   476,  2655,     1,   163,  6422,    66,  6423,  9082,\n",
       "           454,   742,    13,  4419,     8,  3368,  1785,     5, 11679,  6943,\n",
       "             6,  2099,    18,    14,  1817,   350,    27,     1, 13752,    24,\n",
       "             1,   138,   371,    17,    33,   252,    13,   954,     2,   148,\n",
       "             1,   310,    13, 11680,     6,  1602,     3, 13751,     1, 11681,\n",
       "            45,    38,  3467,    33,   742,    38,    17,     1,   631,    43,\n",
       "         16875,  9083,     6,  1602,     3,     1, 16876,    17,    33, 11682,\n",
       "            14,   120,     1,   547,     9,    50,    19,    28,    39,   451,\n",
       "          2656,     3,    11,    29,    10,    13,     1,   472,   541,   372,\n",
       "          4052,    13,   297,     4,     1,   262,   159,    13, 11683,     1,\n",
       "            46,  4224,    37,   205,   718,  9084,    12,     1,   476,   343,\n",
       "            33, 11682,     1,   207,     3,   163,  7558,  2869,  8241,   137,\n",
       "            83,   686,  8240,    55, 23519,     3,   246,  5323,    20,    39,\n",
       "          6944,     6,     1,    65,  1080,   434,   128,  1171,   790,     5,\n",
       "         11684,   491,    45,    38,  8242,     8,     5,  6943,  7559,   152,\n",
       "            48,   492,   152,     4,   119,   152, 23520,    14,    58,   374,\n",
       "           853,  3893, 13753,     7,   800,   942,     7,   442,     2,   416,\n",
       "             1,   310,   632,     5,    67,  2009,     6,   909,    12,   638,\n",
       "             4,  1149,     2,     5,   521,  1053,   103,   991,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaQ7AT01rR1f"
   },
   "source": [
    "These are then used by a `DataLoader` to create batches for training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1732645034408,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "ZPP9oan2b24_",
    "outputId": "718874f7-f27f-4ce0-b2e4-199147e6ba89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 500])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for texts, labels in train_loader:\n",
    "    print(texts.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1732645082943,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "CHB0s1COh1GZ",
    "outputId": "b5b83e00-535f-41e4-cf1e-ef8927ac9929"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5271,   736,     8,  ...,     0,     0,     0],\n",
       "        [ 5609,   853,     7,  ...,     0,     0,     0],\n",
       "        [ 4984,  4286,  2714,  ...,    41,  1530,  7345],\n",
       "        ...,\n",
       "        [  200,  6303,  6894,  ...,   262,   159,   454],\n",
       "        [ 7451,  1153,   142,  ...,     0,     0,     0],\n",
       "        [12244,  1900,   463,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxwXb7cj87bN"
   },
   "source": [
    "### Define the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJPKTVOIYIiL"
   },
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Custom initialization: Set each embedding to have the same value across all dimensions\n",
    "#        custom_weights = torch.zeros((vocab_size, embed_dim))  # Initialize a tensor\n",
    "#        for i in range(vocab_size):\n",
    "#            value = torch.rand(1).item()  # Generate a random value for each embedding\n",
    "#            custom_weights[i, :] = value  # Set all dimensions of the embedding to the same value\n",
    "\n",
    "        # Assign the custom weights to the embedding layer\n",
    "#        self.embedding.weight = nn.Parameter(custom_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embed_dim)\n",
    "        embedded = embedded.permute(0, 2, 1)  # (batch_size, embed_dim, seq_length)\n",
    "        pooled = self.pool(embedded).squeeze(2)  # (batch_size, embed_dim)\n",
    "        output = self.fc(pooled)  # (batch_size, num_classes)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLhQ_1BBrkYX"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 100\n",
    "num_classes = len(label_encoder.classes_)\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1732645289997,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "-i865BjzrqGp",
    "outputId": "d8d50083-eacb-4c49-9e91-a5fc32f457d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 500, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded = embedding(texts)\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1732645331978,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "he34OB0YrqCg",
    "outputId": "8bdfdbea-9caa-4ca3-ca99-c18cbd23b6fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 500])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded = embedded.permute(0,2,1)\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1732645337507,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "Xgtn6U_Mrp_o",
    "outputId": "570becd5-1996-4dbb-a311-3764e287b065"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.AdaptiveAvgPool1d(1)\n",
    "pooled = pool(embedded).squeeze(2)\n",
    "pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1732645392395,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "RMEZlwcqrp8x",
    "outputId": "1859c5cc-3d65-4e3a-a01f-51f3d3ae81d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = nn.Linear(embed_dim, num_classes)\n",
    "output = fc(pooled)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JSnQNCZ8-u3"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93831,
     "status": "ok",
     "timestamp": 1732645541670,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "LsMI6k4cYxjc",
    "outputId": "c63b8887-c460-43a5-9513-0a2ef6b2e2e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.5828\n",
      "Epoch 2/20, Loss: 1.5030\n",
      "Epoch 3/20, Loss: 1.4481\n",
      "Epoch 4/20, Loss: 1.3828\n",
      "Epoch 5/20, Loss: 1.3070\n",
      "Epoch 6/20, Loss: 1.2106\n",
      "Epoch 7/20, Loss: 1.1003\n",
      "Epoch 8/20, Loss: 0.9777\n",
      "Epoch 9/20, Loss: 0.8540\n",
      "Epoch 10/20, Loss: 0.7385\n",
      "Epoch 11/20, Loss: 0.6347\n",
      "Epoch 12/20, Loss: 0.5460\n",
      "Epoch 13/20, Loss: 0.4664\n",
      "Epoch 14/20, Loss: 0.4013\n",
      "Epoch 15/20, Loss: 0.3460\n",
      "Epoch 16/20, Loss: 0.3016\n",
      "Epoch 17/20, Loss: 0.2647\n",
      "Epoch 18/20, Loss: 0.2313\n",
      "Epoch 19/20, Loss: 0.2051\n",
      "Epoch 20/20, Loss: 0.1823\n"
     ]
    }
   ],
   "source": [
    "# Model Parameters\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 100\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "model = TextClassificationModel(vocab_size, embed_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDNQBPAA9Bxa"
   },
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1732645552553,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "OIyGpMZyYm6q",
    "outputId": "6fd72027-7f36-4139-9226-79f67622e85b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9552\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Initialize variables to track predictions and ground truth\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Disable gradient calculation for evaluation\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        # Get model outputs and predictions\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Track total and correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store labels and predictions for confusion matrix\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "classes = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "\n",
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NKRFxBHw9ko"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmbNh2Hq8pFh"
   },
   "source": [
    "### Adaptive Pooling Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1731689974418,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "lhxb_RHpw9Vv",
    "outputId": "bca94dda-3eca-406e-ead7-b4049d0a9a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor:\n",
      "tensor([[[1., 2., 3., 4., 5., 6., 7., 8.]]])\n",
      "\n",
      "Output tensor after applying AdaptiveAvgPool1d:\n",
      "tensor([[[4.5000]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example input tensor of shape (batch_size=1, channels=1, length=8)\n",
    "input_tensor = torch.tensor([[[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]]])\n",
    "print(\"Input tensor:\")\n",
    "print(input_tensor)\n",
    "\n",
    "# Create an AdaptiveAvgPool1d layer that outputs a size of 4\n",
    "adaptive_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "# Apply the layer\n",
    "output_tensor = adaptive_avg_pool(input_tensor)\n",
    "\n",
    "print(\"\\nOutput tensor after applying AdaptiveAvgPool1d:\")\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731689993110,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "iuW-y6wYxAhp",
    "outputId": "8119074d-8dc2-44d4-ba3e-b148c1be7899"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1731690009243,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "6OwP9AUoxFFI",
    "outputId": "8341fe13-936b-4497-fe94-023eb49ec5c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5000]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEzYo-KWxI-w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM+H6d4MSXJ5Nnnp7c3LfIZ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
