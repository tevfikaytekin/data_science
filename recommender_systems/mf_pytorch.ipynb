{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load MovieLens Small dataset\n",
    "data = pd.read_csv(\"../../datasets/ml-latest-small/ratings.csv\", sep=\",\")\n",
    "#data = pd.read_csv(\"../../datasets/ml-1m/ratings.csv\", sep=\",\", names=[\"userId\",\"movieId\",\"rating\",\"timestamp\"])\n",
    "\n",
    "#data = pd.read_csv(\"ratings.csv\", sep=\",\")\n",
    "#prefs = pd.read_csv(\"drive/MyDrive/PycharmProjects/datasets/ml-latest-small/ratings.csv\", sep=\",\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100836, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map user and movie IDs to unique consecutive indices starting from 0.\n",
    "# This is needed because embedding layer indexes start from 0\n",
    "user_ids = data['userId'].unique()\n",
    "movie_ids = data['movieId'].unique()\n",
    "\n",
    "user_mapping = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "movie_mapping = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "data['userId'] = data['userId'].map(user_mapping)\n",
    "data['movieId'] = data['movieId'].map(movie_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the matrix factorization model\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_size=20):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_size)\n",
    "        self.linear = nn.Linear(2*embedding_size, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # forward function takes the indices of some users and movies and make\n",
    "        # prediction by find the dot products of the corresponding users and movies\n",
    "        user_embedding = self.user_embedding(X[:,0])\n",
    "        movie_embedding = self.movie_embedding(X[:,1])\n",
    "        prediction = torch.sum(user_embedding * movie_embedding, dim=1)\n",
    "     \n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.251449108123779\n",
      "Train MAE: 2.9256060138109636\n",
      "Epoch 2/10, Loss: 4.08843469619751\n",
      "Train MAE: 1.347599741776194\n",
      "Epoch 3/10, Loss: 3.7239065170288086\n",
      "Train MAE: 1.008847005199641\n",
      "Epoch 4/10, Loss: 1.9443234205245972\n",
      "Train MAE: 0.8798507344633162\n",
      "Epoch 5/10, Loss: 1.1380342245101929\n",
      "Train MAE: 0.811702022461972\n",
      "Epoch 6/10, Loss: 1.5530986785888672\n",
      "Train MAE: 0.7661024349497313\n",
      "Epoch 7/10, Loss: 0.8333788514137268\n",
      "Train MAE: 0.7359038010776195\n",
      "Epoch 8/10, Loss: 0.6437848806381226\n",
      "Train MAE: 0.7120383922588632\n",
      "Epoch 9/10, Loss: 0.9712590575218201\n",
      "Train MAE: 0.6948130217076073\n",
      "Epoch 10/10, Loss: 0.4200764000415802\n",
      "Train MAE: 0.6801614906525236\n",
      "Elapsed time: 27.504318952560425\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and optimizer\n",
    "num_users = len(user_ids)\n",
    "num_movies = len(movie_ids)\n",
    "model = MatrixFactorization(num_users, num_movies, embedding_size=5)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9)\n",
    "\n",
    "total_loss = 0.0\n",
    "absolute_errors = []\n",
    "num_epochs = 10\n",
    "n_samples = len(train_data)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "# Note1: In order to test the effectives of mini-batching or stochastic gradient descent, \n",
    "# you can try to increase batch_size up to n_samples. In that case the execution time of\n",
    "# each epoch will begin to decrease, however, since updates are done after each batch,\n",
    "# there will be a less number of updates in each epoch which will slow down convergence. On the other hand,\n",
    "# if batch_size is smaller (such as 32) then there will be n_samples/batch_size\n",
    "# updates in each epoch. This type of gradient descent is called mini-batch or stochastic gradient descent.\n",
    "\n",
    "# Note2: When batch_size = n_samples this code will run almost as fast as a C++ code. \n",
    "# However, if you decrease batch size the code will become slower compared to a C++ code\n",
    "# due to the overheads of the torch autograd framework. In the extreme if you set batch_size = 1\n",
    "# this speed difference will be significant.\n",
    "\n",
    "X = torch.tensor(train_data[['userId', 'movieId']].values)\n",
    "y = torch.tensor(train_data['rating'].values.astype(np.float32))\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(n_samples)\n",
    "    absolute_errors = []\n",
    "\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_X, batch_y = X[indices], y[indices]\n",
    "\n",
    "\n",
    "        predictions = model(batch_X)\n",
    "\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        absolute_errors.extend(torch.abs(predictions - batch_y).tolist())\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss}\")\n",
    "        print(f\"Train MAE: {np.mean(absolute_errors)}\")\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time:\", end - start)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"274pt\" height=\"393pt\"\n",
       " viewBox=\"0.00 0.00 274.00 393.25\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 389.25)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-389.25 270,-389.25 270,4 -4,4\"/>\n",
       "<!-- 5020093136 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>5020093136</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"160,-32.75 106,-32.75 106,0 160,0 160,-32.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"133\" y=\"-7.25\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 4996236336 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>4996236336</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"189,-89.5 77,-89.5 77,-68.75 189,-68.75 189,-89.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"133\" y=\"-76\" font-family=\"monospace\" font-size=\"10.00\">MseLossBackward0</text>\n",
       "</g>\n",
       "<!-- 4996236336&#45;&gt;5020093136 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>4996236336&#45;&gt;5020093136</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133,-68.36C133,-61.89 133,-53.05 133,-44.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.5,-44.55 133,-34.55 129.5,-44.55 136.5,-44.55\"/>\n",
       "</g>\n",
       "<!-- 4996236288 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>4996236288</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"177,-146.25 89,-146.25 89,-125.5 177,-125.5 177,-146.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"133\" y=\"-132.75\" font-family=\"monospace\" font-size=\"10.00\">SumBackward1</text>\n",
       "</g>\n",
       "<!-- 4996236288&#45;&gt;4996236336 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>4996236288&#45;&gt;4996236336</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133,-125.09C133,-118.47 133,-109.47 133,-101.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.5,-101.34 133,-91.34 129.5,-101.34 136.5,-101.34\"/>\n",
       "</g>\n",
       "<!-- 4996236864 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4996236864</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"177,-203 89,-203 89,-182.25 177,-182.25 177,-203\"/>\n",
       "<text text-anchor=\"middle\" x=\"133\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 4996236864&#45;&gt;4996236288 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4996236864&#45;&gt;4996236288</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133,-181.84C133,-175.22 133,-166.22 133,-158.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.5,-158.09 133,-148.09 129.5,-158.09 136.5,-158.09\"/>\n",
       "</g>\n",
       "<!-- 4996236912 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4996236912</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"124,-259.75 0,-259.75 0,-239 124,-239 124,-259.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-246.25\" font-family=\"monospace\" font-size=\"10.00\">EmbeddingBackward0</text>\n",
       "</g>\n",
       "<!-- 4996236912&#45;&gt;4996236864 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4996236912&#45;&gt;4996236864</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M74.69,-238.59C84.89,-230.72 99.44,-219.5 111.46,-210.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"113.44,-213.13 119.22,-204.25 109.16,-207.59 113.44,-213.13\"/>\n",
       "</g>\n",
       "<!-- 4996236720 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4996236720</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"112,-316.5 12,-316.5 12,-295.75 112,-295.75 112,-316.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-303\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 4996236720&#45;&gt;4996236912 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4996236720&#45;&gt;4996236912</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M62,-295.34C62,-288.72 62,-279.72 62,-271.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.5,-271.59 62,-261.59 58.5,-271.59 65.5,-271.59\"/>\n",
       "</g>\n",
       "<!-- 4995755248 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>4995755248</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"97,-385.25 27,-385.25 27,-352.5 97,-352.5 97,-385.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-359.75\" font-family=\"monospace\" font-size=\"10.00\"> (610, 5)</text>\n",
       "</g>\n",
       "<!-- 4995755248&#45;&gt;4996236720 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4995755248&#45;&gt;4996236720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M62,-352.23C62,-344.85 62,-335.93 62,-327.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.5,-328.16 62,-318.16 58.5,-328.16 65.5,-328.16\"/>\n",
       "</g>\n",
       "<!-- 4996236384 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>4996236384</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"266,-259.75 142,-259.75 142,-239 266,-239 266,-259.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"204\" y=\"-246.25\" font-family=\"monospace\" font-size=\"10.00\">EmbeddingBackward0</text>\n",
       "</g>\n",
       "<!-- 4996236384&#45;&gt;4996236864 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4996236384&#45;&gt;4996236864</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M191.31,-238.59C181.11,-230.72 166.56,-219.5 154.54,-210.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.84,-207.59 146.78,-204.25 152.56,-213.13 156.84,-207.59\"/>\n",
       "</g>\n",
       "<!-- 4996236768 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>4996236768</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"254,-316.5 154,-316.5 154,-295.75 254,-295.75 254,-316.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"204\" y=\"-303\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 4996236768&#45;&gt;4996236384 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>4996236768&#45;&gt;4996236384</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M204,-295.34C204,-288.72 204,-279.72 204,-271.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"207.5,-271.59 204,-261.59 200.5,-271.59 207.5,-271.59\"/>\n",
       "</g>\n",
       "<!-- 4995750352 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>4995750352</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"242,-385.25 166,-385.25 166,-352.5 242,-352.5 242,-385.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"204\" y=\"-359.75\" font-family=\"monospace\" font-size=\"10.00\"> (9724, 5)</text>\n",
       "</g>\n",
       "<!-- 4995750352&#45;&gt;4996236768 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>4995750352&#45;&gt;4996236768</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M204,-352.23C204,-344.85 204,-335.93 204,-327.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"207.5,-328.16 204,-318.16 200.5,-328.16 207.5,-328.16\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x129ca5f10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0\n",
      "Test MAE: 0.8452265566183148\n",
      "Test MAE: 0.8452265858650208\n",
      "Test RMSE: 1.2780202627182007\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.tensor(test_data[['userId', 'movieId']].values)\n",
    "y_test = torch.tensor(test_data['rating'].values.astype(np.float32))\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "absolute_errors = []\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "    loss = criterion(predictions, y_test)\n",
    "    \n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(X_test)}\")\n",
    "print(f\"Test MAE: {np.abs((predictions - y_test).tolist()).mean()}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(y_test, predictions)}\")\n",
    "print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, predictions))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe[['userId', 'movieId']].values\n",
    "        self.ratings = dataframe['rating'].values.astype(np.float32)\n",
    "        \n",
    "        # It is faster (at least 2x) to use numpy arrays instead of a dataframe as below\n",
    "        # and access the data in __getitem__ using iloc\n",
    "        #self.data = dataframe[['userId', 'movieId']]\n",
    "        #self.ratings = dataframe['rating']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.data[idx]), torch.FloatTensor([self.ratings[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.1)\n",
    "\n",
    "# Initialize dataset and data loaders\n",
    "train_dataset = MovieLensDataset(train_data)\n",
    "test_dataset = MovieLensDataset(test_data)\n",
    "\n",
    "n_samples = len(train_data)\n",
    "# Note: Setting batch_size larger values will decrease the time of a single epoch but after\n",
    "# a certain value it does not further decrease. In the extreme even if you set batch_size to n_samples\n",
    "# the running time of a single epoch will still be significant. This is probably because of the overhead\n",
    "# sourced by dataset loader.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu.\n",
      "Epoch 1/10, Loss: 12.102086466105263\n",
      "Train MAE: 2.901769384524387\n",
      "Epoch 2/10, Loss: 3.5102003144024123\n",
      "Train MAE: 1.353004370287011\n",
      "Epoch 3/10, Loss: 2.087010719743787\n",
      "Train MAE: 1.0229356126174756\n",
      "Epoch 4/10, Loss: 1.5477632239608066\n",
      "Train MAE: 0.8890790826948399\n",
      "Epoch 5/10, Loss: 1.2585167464956495\n",
      "Train MAE: 0.8130854773423825\n",
      "Epoch 6/10, Loss: 1.0828765742901099\n",
      "Train MAE: 0.7653017575768505\n",
      "Epoch 7/10, Loss: 0.961940820698005\n",
      "Train MAE: 0.7306597642784436\n",
      "Epoch 8/10, Loss: 0.8742180279231458\n",
      "Train MAE: 0.7035524221091191\n",
      "Epoch 9/10, Loss: 0.8109221578705714\n",
      "Train MAE: 0.6841327615974836\n",
      "Epoch 10/10, Loss: 0.7630794320113652\n",
      "Train MAE: 0.6670970934875341\n",
      "Elapsed time: 51.30553197860718\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and optimizer\n",
    "num_users = len(user_ids)\n",
    "num_movies = len(movie_ids)\n",
    "model = MatrixFactorization(num_users, num_movies, embedding_size=10)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {device}.\")\n",
    "#model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    absolute_errors = []\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        #X = X.to(device)\n",
    "        #y = y.to(device)\n",
    "        predictions = model(X)\n",
    "        loss = criterion(predictions, y.squeeze())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        absolute_errors.extend(torch.abs(predictions - y.squeeze()).tolist())\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "    print(f\"Train MAE: {np.mean(absolute_errors)}\")\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time:\", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7731850219395342\n",
      "Test MAE: 0.8969225770291805\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the test set\n",
    "test_loss = 0.0\n",
    "absolute_errors = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (X, y) in enumerate(test_loader):\n",
    "        predictions = model(X)\n",
    "        loss = criterion(predictions, y.squeeze())\n",
    "        test_loss += loss.item()\n",
    "        absolute_errors.extend(torch.abs(predictions - y.squeeze()).tolist())\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader)}\")\n",
    "print(f\"Test MAE: {np.mean(absolute_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is going on under the hood?\n",
    "\n",
    "Some explanations in order to understand what is going on during the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "num_users = len(user_ids)\n",
    "num_movies = len(movie_ids)\n",
    "model = MatrixFactorization(num_users, num_movies, embedding_size=5)\n",
    "n_samples = len(train_data)\n",
    "X = torch.tensor(train_data[['userId', 'movieId']].values)\n",
    "y = torch.tensor(train_data['rating'].values.astype(np.float32))\n",
    "permutation = torch.randperm(n_samples)\n",
    "indices = permutation[0:0+32]\n",
    "batch_X, batch_y = X[indices], y[indices]\n",
    "predictions = model(batch_X)\n",
    "loss = criterion(predictions, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X contains the indices of pairs of users and movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 42, 343],\n",
       "        [302, 757],\n",
       "        [501,  71]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a set of users and movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "uidx = torch.tensor([0,1,2])\n",
    "iidx = torch.tensor([6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0025, -0.4543, -0.7436,  1.6553,  1.9770],\n",
       "        [-0.1971,  0.0846, -0.6548,  1.4419,  1.4326],\n",
       "        [-0.8921, -1.0224, -0.3187,  0.2523,  0.4210]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_emb = model.user_embedding(uidx)\n",
    "user_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0895, -1.1188, -0.7012,  1.8861,  0.7449],\n",
       "        [-0.8070, -0.8636, -0.1842,  0.4362,  1.6826],\n",
       "        [-0.1349, -0.9455, -1.8755,  0.2066,  1.1307]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_emb = model.movie_embedding(iidx)\n",
    "movie_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find their dot product by first element-wise multiplication then sum along the x dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0897,  0.5083,  0.5214,  3.1219,  1.4727],\n",
       "        [ 0.1591, -0.0731,  0.1206,  0.6290,  2.4105],\n",
       "        [ 0.1204,  0.9667,  0.5977,  0.0521,  0.4760]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mul = user_emb * movie_emb\n",
    "mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.7140, 3.2461, 2.2128], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(mul, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.7139999999999995"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ 0.0897,  0.5083,  0.5214,  3.1219,  1.4727])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
