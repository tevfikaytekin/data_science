{"cells":[{"cell_type":"markdown","id":"4feb50c7","metadata":{"id":"4feb50c7"},"source":["# Neighborhood-based Collaborative Filtering\n","(by Tevfik Aytekin)\n","\n","In the following we will implement item based collaborative filtering."]},{"cell_type":"code","execution_count":null,"id":"510f6742","metadata":{"id":"510f6742"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","from scipy.sparse import csr_matrix\n","from collections import Counter\n","from sklearn.metrics import pairwise_distances\n","from operator import itemgetter\n","from tqdm.notebook import tqdm\n","import copy\n","import heapq\n","import sys, os\n","import pickle\n","import itertools\n","import operator\n"]},{"cell_type":"code","metadata":{"id":"db9bd9c6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"db9bd9c6","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c381eb7b","metadata":{"id":"c381eb7b"},"source":["### Movielens ml-latest-small dataset"]},{"cell_type":"code","execution_count":null,"id":"36dd568b","metadata":{"id":"36dd568b"},"outputs":[],"source":["with open('/content/drive/My Drive/datasets/ml-latest-small/README.txt', 'r') as f:\n","    print(f.read())"]},{"cell_type":"code","execution_count":null,"id":"2e28b48c","metadata":{"id":"2e28b48c"},"outputs":[],"source":["ratings = pd.read_csv(\"/content/drive/My Drive/datasets/ml-latest-small/ratings.csv\", sep=\",\")\n","#ratings =pd.read_csv(\"/content/drive/My Drive/datasets/ml-25m/ratings.csv\")\n","print(ratings.shape)\n","ratings.head(10)"]},{"cell_type":"code","execution_count":null,"id":"f438a1e0","metadata":{"id":"f438a1e0"},"outputs":[],"source":["links = pd.read_csv(\"/content/drive/My Drive/datasets/ml-latest-small/links.csv\", sep=\",\")\n","print(links.shape)\n","links.head()"]},{"cell_type":"code","execution_count":null,"id":"afd90fe2","metadata":{"id":"afd90fe2"},"outputs":[],"source":["movies = pd.read_csv(\"/content/drive/My Drive/datasets/ml-latest-small/movies.csv\", sep=\",\")\n","print(movies.shape)\n","movies.head()"]},{"cell_type":"code","execution_count":null,"id":"ddcc9382","metadata":{"id":"ddcc9382"},"outputs":[],"source":["tags = pd.read_csv(\"/content/drive/My Drive/datasets/ml-latest-small/tags.csv\", sep=\",\")\n","print(tags.shape)\n","tags.head()"]},{"cell_type":"markdown","id":"71d3800e","metadata":{"id":"71d3800e"},"source":["## Create User Item Rating Map\n","It might take some time but will be useful later."]},{"cell_type":"code","execution_count":null,"id":"749b4bb1","metadata":{"id":"749b4bb1"},"outputs":[],"source":["rating_map = {}\n","for i in range(len(ratings)):\n","    key = str(ratings.iloc[i,0]) + '_' +str(ratings.iloc[i,1])\n","    rating_map[key]=ratings.iloc[i,2]"]},{"cell_type":"code","execution_count":null,"id":"fd29dac0","metadata":{"id":"fd29dac0"},"outputs":[],"source":["rating_map[\"1_101\"]"]},{"cell_type":"code","execution_count":null,"id":"dbb143c2","metadata":{"id":"dbb143c2"},"outputs":[],"source":["iterator = iter(rating_map.items())\n","for i in range(5):\n","    print(next(iterator))"]},{"cell_type":"markdown","id":"19f1284f","metadata":{"id":"19f1284f"},"source":["## Create User Ratings Map\n","It might take some time but will be useful later."]},{"cell_type":"code","execution_count":null,"id":"470298bb","metadata":{"id":"470298bb"},"outputs":[],"source":["ratings.query(\"movieId == 2\")"]},{"cell_type":"code","execution_count":null,"id":"fb51140e","metadata":{"id":"fb51140e"},"outputs":[],"source":["# user_ratings_map[i] stores a tuple: a list of users who rated item i and a list of corresponding ratings\n","user_ratings_map = {}\n","\n","items = ratings.movieId.unique()\n","for i in items:\n","    userids = ratings.query(\"movieId == @i\").userId.array\n","    user_ratings = ratings.query(\"movieId == @i\").rating.array\n","    user_ratings_map[i] = (userids,user_ratings)"]},{"cell_type":"code","execution_count":null,"id":"c918ce66","metadata":{"id":"c918ce66"},"outputs":[],"source":["user_ratings_map[10]"]},{"cell_type":"markdown","id":"ec5d5454","metadata":{"id":"ec5d5454"},"source":["## Rating Prediction\n","\n","###Â Algorithm\n","\n","Predict rating of user $u$ for item $i$\n","- Calculate the similarity of items that are rated by $u$ with $i$.\n","- Use these similarities to calculate a weighted average of the ratings.\n","\n","Similarity between items i and j will be calculated using the ratings of i and j (no content information will be used). One can view these ratings as a vector of values as shown below and use different metrics such as Jaccard or cosine."]},{"cell_type":"code","execution_count":null,"id":"b761853a","metadata":{"id":"b761853a"},"outputs":[],"source":["df = pd.DataFrame([[1, \"\", 5, 3, \"\"],\n","                   [5, 3, \"\", 2, 4],\n","                   [\"\", 4, 2, \"\", 1],\n","                   [3, \"\", 4, 2, 3],\n","                   [4, 1, \"\", 2, 4],\n","                   [4, 1, 5, 3, \"\"],\n","                   [\"\", 4, 5, \"\", 1],\n","                   [2, 5, \"\", 1, 4]],\n","                 index = ['user 1','user 2','user 3','user 4','user 5','user 6','user 7','user 8'],\n","                 columns = ['movie 1','movie 2','movie 3','movie 4',' movie 5'])\n","df"]},{"cell_type":"markdown","id":"b0e553da","metadata":{"id":"b0e553da"},"source":["### Jaccard Similarity\n","\n","Given two sets $A$ and $B$,\n","\n","$Jaccard(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$\n","\n","For example if $A = \\{a, b, c, d\\}$ and $B = \\{b, d, e ,f, g\\}$ then\n","\n","$Jaccard(A, B) = \\frac{2}{7}$\n","\n","We can apply Jaccard similarity by ignoring the rating values.\n","\n","### Cosine Similarity\n","\n","Cosine(A, B) =\n","\n","<img src=\"cosine.png\" width=\"200\">\n","\n"]},{"cell_type":"code","execution_count":null,"id":"37328e31","metadata":{"id":"37328e31"},"outputs":[],"source":["def NNCF_based_rating_prediction(u, i, metric):\n","    r = 0\n","    sum_sim = 0\n","    # find movies rated by u\n","    movies = ratings[ratings[\"userId\"]==u].movieId\n","    for j in movies:\n","        sim = calc_sim(i, j, metric)\n","        key = str(u)+\"_\"+str(j)\n","        r += sim*rating_map[key]\n","        sum_sim += sim\n","    if sum_sim == 0:\n","        return 0\n","    else:\n","        return r / sum_sim"]},{"cell_type":"code","execution_count":null,"id":"bbf6dafb","metadata":{"id":"bbf6dafb"},"outputs":[],"source":["# finds the similary of items i and j\n","def calc_sim(i,j, metric):\n","    # users who rated item i\n","    users_rated_i = user_ratings_map[i][0]\n","    ratings_i = user_ratings_map[i][1]\n","    # users who rated item j\n","    users_rated_j = user_ratings_map[j][0]\n","    ratings_j = user_ratings_map[j][1]\n","\n","    # Jaccard ignores rating values.\n","    if metric == \"Jaccard\":\n","        intersection_size = len(set(users_rated_i).intersection(users_rated_j))\n","        union_size = len(set(users_rated_i).union(users_rated_j))\n","        return intersection_size / union_size\n","    elif metric == \"Cosine\":\n","        inter, ind1, ind2 = np.intersect1d(users_rated_i, users_rated_j, return_indices=True)\n","        dot = np.dot(ratings_i[ind1], ratings_j[ind2])\n","        return dot/(np.linalg.norm(ratings_i[ind1])*np.linalg.norm(ratings_j[ind2]))\n"]},{"cell_type":"markdown","id":"f9ca7a8e","metadata":{"id":"f9ca7a8e"},"source":["### Example"]},{"cell_type":"code","execution_count":null,"id":"470f3317","metadata":{"id":"470f3317"},"outputs":[],"source":["users_i = np.array([1, 3, 6, 9, 12, 15])\n","ratings_i =  np.array([4, 3, 2, 4, 2, 5])\n","users_j = np.array([1, 6, 8, 12])\n","ratings_j = np.array([2, 4, 2, 5])"]},{"cell_type":"code","execution_count":null,"id":"55203535","metadata":{"id":"55203535"},"outputs":[],"source":["inter, ind1, ind2 = np.intersect1d(users_i,users_j,return_indices=True)\n","print(inter)\n","print(ind1)\n","print(ind2)\n","print(ratings_i[ind1])\n","print(ratings_j[ind2])\n","print(np.dot(ratings_i[ind1],ratings_j[ind2]))"]},{"cell_type":"markdown","id":"6ce68b8d","metadata":{"id":"6ce68b8d"},"source":["## Evaluation of Rating Prediction\n","\n","How can we measure the performance of a recommender algorithm? This is similar to the evaluation used in machine learning.\n","\n","- Make a train/test split\n","- Build the model on the training set\n","- Make predictions for the ratings in the test set\n","- Find the mean absolute error (MAE)\n","\n","For more metrics other then MAE lool at the \"Metrics for Regression\" section of [this notebook](../../data_science/evaluation.ipynb). For ranking metrics see [this notebook](ranking_evaluation.ipynb)\n"]},{"cell_type":"code","execution_count":null,"id":"11bcab01","metadata":{"id":"11bcab01"},"outputs":[],"source":["ratings = shuffle(ratings)\n","\n","X_train, X_test = train_test_split(ratings, test_size=100)\n","train_size = X_train.shape[0]\n","test_size = X_test.shape[0]\n","print(\"Test size:\", test_size)\n","error1 = 0\n","error2 = 0\n","error3 = 0\n","preds = []\n","\n","avg_rating = X_train.iloc[:,2].mean()\n","for k in tqdm(range(test_size)):\n","    u = X_test.iloc[k,0]\n","    i = X_test.iloc[k,1]\n","    r = X_test.iloc[k,2]\n","\n","    error1 += np.abs(r - NNCF_based_rating_prediction(u,i,\"Cosine\"))\n","    error2 += np.abs(r - NNCF_based_rating_prediction(u,i,\"Jaccard\"))\n","    error3 += np.abs(r - avg_rating)\n","\n","\n","print(\"Cosine:\", error1/test_size)\n","print(\"Jaccard:\",error2/test_size)\n","print(\"Average:\",error3/test_size)\n"]},{"cell_type":"markdown","id":"b12cb3bf","metadata":{"id":"b12cb3bf"},"source":["#### Question: How can you explain the difference between using Cosine vs. Jaccard?"]},{"cell_type":"markdown","id":"c2eb0801","metadata":{"id":"c2eb0801"},"source":["## Top-N recommendation Algorithm - Predict and Sort\n","The task in top-$N$ recommendation is to recommend $N$ items to a user.\n","\n","\n","Recommend $N$ movies to user $u$\n","- Predict the ratings of all items which are not watched by $u$\n","- Sort the predicted ratings\n","- Recommend the movies with the highest predicted ratings"]},{"cell_type":"code","execution_count":null,"id":"bb1d7323","metadata":{"id":"bb1d7323"},"outputs":[],"source":["def top_N_pred_sort(N, u):\n","    preds = pd.Series([], dtype='float')\n","    # find the movies not rated by u\n","    all_items = set(ratings['movieId'].unique())\n","    rated_by_user = set(ratings[ratings[\"userId\"]==1].movieId.unique())\n","    not_rated_by_user = all_items - rated_by_user\n","    sample_movies = np.random.choice(list(not_rated_by_user), 500)\n","    for m in tqdm(sample_movies):\n","        preds[m] = NNCF_based_rating_prediction(u, m, \"Jaccard\")\n","    return preds.sort_values(ascending=False)[:N]"]},{"cell_type":"code","execution_count":null,"id":"975cd7e1","metadata":{"id":"975cd7e1"},"outputs":[],"source":["topn = top_N_pred_sort(10, 100)\n","topn"]},{"cell_type":"markdown","id":"dc9766a2","metadata":{"id":"dc9766a2"},"source":["## Efficiency Issues\n","\n","There are important inefficiencies in this algorithm:\n","\n","- The algorithm predicts the rating of all items which are not rated by the user. In the case of millions of items this algorithm is practically infeasible. Numerous techniques have been developed to remedy this problem. Can you suggest a solution?\n","- In rating prediction, similarity between target item and items rated by the user are calculated. To make a recommendation to another user similarity calculations will be done again. For making recommendations to users in general many similarity calculations will be repeated. A general solution to this problem is to precalculate the similarities between items. Moreover, you don't need to store all similarities, only storing $k$ most similar items to every item will be enough. Size of $k$ can be determined according to the needs.\n"]},{"cell_type":"markdown","id":"934589c6","metadata":{"id":"934589c6"},"source":["## Top-N recommendation Algorithm - kNN Map\n","The task in top-$N$ recommendation is to recommend $N$ items to a user.\n","\n","- Build a knn-map (a map which stores the $k$ nearest neighbors of each item)\n","\n","Recommend $N$ movies to user $u$\n","- Get the neigbors of movies which are watched by $u$ and put them into a list $C$.\n","- Choose $N$ movies form $C$. There can be different methods here. Most repeated movies in C can be chosen, movies with the highest total similarity (or maximum similarity) can be chosen. These methods will be implemented.\n","- Recommend the $N$ movies that are chosen."]},{"cell_type":"markdown","id":"008e77bc","metadata":{"id":"008e77bc"},"source":["## Building a knn map\n","This table will hold the most similar $k$ items for each item. In order to build this table we need to calculate all pairwise similarities which takes $O(n^2)$ time. There is no escape from this $O(n^2)$ time unless you use an approximation algorithm such as LSH (Locality Sensitive Hashing) for nearest neighbor search. Another approach might be to calculate similarites by a matrix multiplication operation and give it to a GPU for accelaration. Yet another approach is discovered by Amazon researchers which can lead to huge speed ups for very sparse matrices which we will look at below.\n","\n","We will use a heap based priority queue for storing the nearest neighbor. You can look at this [animation](https://www.cs.usfca.edu/~galles/visualization/Heap.html)."]},{"cell_type":"code","execution_count":null,"id":"226648ab","metadata":{"id":"226648ab"},"outputs":[],"source":["pq =[(10,\"a\"),(8, \"b\"), (5, \"c\"), (3, \"d\")]\n","type(pq)\n","heapq.heapify(pq)\n","heapq.nsmallest(2,pq)"]},{"cell_type":"code","execution_count":null,"id":"8c5f6748","metadata":{"id":"8c5f6748"},"outputs":[],"source":["movies[:10]"]},{"cell_type":"code","execution_count":null,"id":"c44ad103","metadata":{"scrolled":true,"id":"c44ad103"},"outputs":[],"source":["def build_knn_map(movies, K=30):\n","    knn_map = {}\n","    movie_ids = ratings['movieId'].unique()\n","    print(len(movie_ids))\n","    for i in tqdm(movie_ids):\n","        pq = []\n","        knn_map[i] = pq\n","        for j in movie_ids:\n","            if (i == j):\n","                continue\n","            sim = calc_sim(i,j,\"Jaccard\")\n","            if (len(pq) >= K):\n","                smallest = pq[0]\n","                if (sim > smallest[0]):\n","                    heapq.heappop(pq)\n","                    heapq.heappush(pq, (sim, j))\n","            else:\n","                heapq.heappush(pq, (sim, j))\n","    return knn_map"]},{"cell_type":"code","execution_count":null,"id":"e9d0b4dc","metadata":{"id":"e9d0b4dc"},"outputs":[],"source":["knn_map = build_knn_map(movies)"]},{"cell_type":"markdown","metadata":{"id":"6Wuncl-iJ23y"},"source":["### Amazon Algorithm\n","\n","The following is the algorithm used at Amazon. Note that this does not help much for the movielens dataset, however, for very sparse datasets such as the dataset at Amazon, it really helps by skipping many item pairs for which there is no user which rated/bought both items.\n","\n","Linden, Greg, Brent Smith, and Jeremy York. \"Amazon. com recommendations: Item-to-item collaborative filtering.\" IEEE Internet computing 7.1 (2003): 76-80.\n"],"id":"6Wuncl-iJ23y"},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzF5rCM3J23y"},"outputs":[],"source":["def build_knn_map_amazon(movies, K=30):\n","    knn_map = {}\n","    movie_ids = ratings['movieId'].unique()\n","    print(len(movie_ids))\n","    for i in tqdm(movie_ids):\n","        pq = []\n","        knn_map[i] = pq\n","        # find users who rated i\n","        users = ratings.query(\"movieId == @i\").userId.unique()\n","        # find items rated by users_i\n","        movies = ratings.query(\"userId in @users\").movieId.unique()\n","        # For speed up (this does not exist in the Amazon algorithm)\n","        movies = np.random.choice(movies, 500)\n","        for j in movies:\n","            if (i == j):\n","                continue\n","            sim = calc_sim(i,j,\"Jaccard\")\n","            if (len(pq) >= K):\n","                smallest = pq[0]\n","                if (sim > smallest[0]):\n","                    heapq.heappop(pq)\n","                    heapq.heappush(pq, (sim, j))\n","            else:\n","                heapq.heappush(pq, (sim, j))\n","    return knn_map, movies"],"id":"JzF5rCM3J23y"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Zn3KhTgJ232"},"outputs":[],"source":["knn_map, movies = build_knn_map_amazon(movies)"],"id":"3Zn3KhTgJ232"},{"cell_type":"code","execution_count":null,"id":"a2dc618d","metadata":{"id":"a2dc618d"},"outputs":[],"source":["def add_sims_and_sort(l):\n","    li = []\n","    it = itertools.groupby(l, operator.itemgetter(1))\n","    for key, subiter in it:\n","        li.append((key, sum(item[0] for item in subiter)))\n","    li = sorted(li, key=itemgetter(1), reverse=True)\n","    return li\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jrcc2GNMJ233"},"outputs":[],"source":["def top_N_knn_map(ratings, N, u):\n","    C = []\n","    # find the movies rated by u\n","    movies_rated = ratings.query(\"userId == @u\").movieId\n","    for m in movies_rated:\n","        C = C + knn_map[m]\n","    return add_sims_and_sort(C)[:N]"],"id":"Jrcc2GNMJ233"},{"cell_type":"code","execution_count":null,"id":"04cffbcc","metadata":{"id":"04cffbcc"},"outputs":[],"source":["topn = top_N_knn_map(ratings, 10, 100)\n","topn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4H17DqVJJ233"},"outputs":[],"source":["topn = [i[0] for i in topn]\n","movies[movies.movieId.isin(topn)]"],"id":"4H17DqVJJ233"},{"cell_type":"markdown","id":"8666ebda","metadata":{"id":"8666ebda"},"source":["## Evaluation of top-N recommendation\n","\n","Evaluation of rating prediction is rather easy: find the mean absolute error between rating predictions and true ratings. How can we evaluate the accuracy of a top-N recommendation? There are several techniques which we will look at in more detail later. Below is one common way to evaluate top-N recommendation:\n","\n","- Randomly sub-sample some portion of positive preferences in order to create a test set $T$. Positive preferences might be 5-star ratings, movies watched more than a certain threshold, or items purchased.\n","- Put the rest of the preferences into the training set and build model.\n","\n","- For each preference $(u,i)$ in the test set:\n","    - Make a top-N recommendation tu user $u$.\n","    - If the test item i occurs among the top-N items, then we have a hit, otherwise we have a miss.\n","\n","Hit ratio is then defined as:\n","\n","$$\n","Hit Ratio: \\frac{\\#hits}{|T|}\n","$$\n","\n"]},{"cell_type":"code","execution_count":null,"id":"c646b5cd","metadata":{"id":"c646b5cd"},"outputs":[],"source":["N = 100\n","X_train, X_test = train_test_split(ratings, test_size=1000)\n","X_test = X_test.query(\"rating > 4\")\n","train_size = X_train.shape[0]\n","test_size = X_test.shape[0]\n","print(\"Test size:\", test_size)\n","hit_count = 0\n","for k in range(test_size):\n","    u = X_test.iloc[k,0]\n","    i = X_test.iloc[k,1]\n","    r = X_test.iloc[k,2]\n","    top_N = top_N_knn_map(X_train, N, u)\n","    hit_list = [item for item in top_N if item[0] == i]\n","    if len(hit_list) > 0:\n","        hit_count +=1\n","print(\"Hit Ratio\", hit_count/test_size)"]},{"cell_type":"markdown","source":["### OPTIONAL\n","\n","The following is a comparison of the running times of similarity calculations performed on the CPU and GPU.\n","\n","https://github.com/erogluegemen/Performance-and-Scalability-Analysis-of-KNN-Implementations-for-Content-Based-Filtering"],"metadata":{"id":"TSHAmwyF4k3d"},"id":"TSHAmwyF4k3d"},{"cell_type":"code","execution_count":null,"id":"4d70b304","metadata":{"id":"4d70b304"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"sklearn-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}