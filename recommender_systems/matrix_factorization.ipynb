{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tevfikaytekin/data_science/blob/master/recommender_systems/matrix_factorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30867,
     "status": "ok",
     "timestamp": 1663598923375,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "s-VES7eiOHSU",
    "outputId": "079e4a0d-7130-4ef2-b540-10eea84f97fb"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgs5IIoUSn9l"
   },
   "source": [
    "# Matrix Factorization\n",
    "(by Tevfik Aytekin)\n",
    "\n",
    "Matrix factorizarion is one of the state-of-the-art techniques used in recommender systems. Below you can find several different implementations.\n",
    "\n",
    "Over the years many variations of matrix factorization have been proposed. The following formulation is one of the simplest but which works well as we will see. It can be extended it various ways, see for example [Advances in Collaborative Filtering](https://link.springer.com/chapter/10.1007/978-1-4899-7637-6_3).\n",
    "\n",
    "Cost Function:\n",
    "$$\n",
    "J(\\Theta) =  \\sum_{u,i \\in K} (r_{ui} - q^T_ip_u)^2 + \\lambda(||q_i||^2+||p_u||^2)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $r_{ui}$ is the rating of user $u$ for item $i$.\n",
    "- $K$ is the set of $(u,i)$ pairs for which $r_{ui}$ is known.\n",
    "- $q_i$, $p_u$ are latent factor vectors for items and users, respectively. \n",
    "- $\\lambda$ is the regularization parameter.\n",
    "\n",
    "And the optimization objective:\n",
    "\n",
    "$$\n",
    "\\min_{p*,q*} \\sum_{u,i \\in K} (r_{ui} - q^T_ip_u)^2 + \\lambda(||q_i||^2+||p_u||^2)\n",
    "$$\n",
    "\n",
    "Typically the optimization done with gradient descent. To apply it we need to first find the partial derivative of the cost function with respect to latent variables which we will denote as $q_{fi}$ and $p_{fu}$. We can find the partial derivative as: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial p_{ku}}=-\\sum_{i \\in I_u}2(r_{ui} - q^T_ip_u)q_{ki} + 2\\lambda p_{ku}\n",
    "$$\n",
    "\n",
    "For **stochastic gradient descent** the update rule for the the $p_u$ vector for a single training example is:\n",
    "\n",
    "$$\n",
    "p_u = p_u + \\alpha ((r_{ui} - q^T_ip_u)q_{i} - \\lambda p_{u})\n",
    "$$\n",
    "\n",
    "similarly for $q_i$ vector we have:\n",
    "\n",
    "$$\n",
    "q_i = q_i + \\alpha ((r_{ui} - q^T_ip_u)p_{u} - \\lambda q_{u})\n",
    "$$\n",
    "\n",
    "For **batch gradient descent** the update rule for the the $p_u$ vector for all preferences where user $u$ appears:\n",
    "\n",
    "$$\n",
    "p_u = p_u + \\alpha (\\sum_{i \\in I_u}(r_{ui} - q^T_ip_u)q_{i} - \\lambda p_{u})\n",
    "$$\n",
    "\n",
    "similarly for $q_i$ vector we have:\n",
    "\n",
    "$$\n",
    "q_i = q_i + \\alpha (\\sum_{u \\in U_i} (r_{ui} - q^T_ip_u)p_{u} - \\lambda q_{u})\n",
    "$$\n",
    "\n",
    "In the above equations $I_u$ is the set of items rated by user $u$ and $U_i$ is the set of users who rated item $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "84apIe07Sn9q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.sparse import csr_matrix\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzo9HTNYSn9r"
   },
   "source": [
    "# Movielens dataset\n",
    "\n",
    "We will use the smallest Movielens 100k Dataset which includes 100k preferences. A preference is a triple (user, item, rating). You can download this data set from\n",
    "[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "Note the sparsity of the dataset which shows that most of the user/item matrix is empty. This is a typical property of the datasets in this domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1663599210587,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "JvZEZ2KFSn9r",
    "outputId": "b26ea5cb-8036-42b4-8243-1874704336ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefs = pd.read_csv(\"../../datasets/ml-latest-small/ratings.csv\", sep=\",\")\n",
    "#prefs = pd.read_csv(\"ratings.csv\", sep=\",\")\n",
    "#prefs = pd.read_csv(\"drive/MyDrive/PycharmProjects/datasets/ml-latest-small/ratings.csv\", sep=\",\")\n",
    "\n",
    "prefs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1663599222262,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "KNUNaoQSSn9s",
    "outputId": "244822bf-72ab-4131-fec1-f0ad8ef0a30f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 610\n",
      "Number of items: 9724\n",
      "Number of preferences: 100836\n",
      "Sparsity: 0.016999683055613623\n"
     ]
    }
   ],
   "source": [
    "n_users = prefs.iloc[:,0].unique().size\n",
    "n_items = prefs.iloc[:,1].unique().size\n",
    "n_prefs = prefs.iloc[:,1].size\n",
    "n_factors = 2\n",
    "users = prefs.iloc[:,0].unique()\n",
    "items = prefs.iloc[:,1].unique()\n",
    "\n",
    "print(\"Number of users:\",n_users)\n",
    "print(\"Number of items:\",n_items)\n",
    "print(\"Number of preferences:\",n_prefs)\n",
    "print(\"Sparsity:\",n_prefs/(n_users*n_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A6dD4keSn9t"
   },
   "source": [
    "### Error Function\n",
    "\n",
    "Error is calculated by predicting the rating of a user and an item in the test set using the factor representations of users and items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R6xzCHN4Sn9t"
   },
   "outputs": [],
   "source": [
    "def calc_error(X, u_factors, i_factors):\n",
    "    error = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        u_idx = X.iloc[i,0]\n",
    "        i_idx = X.iloc[i,1]\n",
    "        error += np.abs(X.iloc[i,2] - np.dot(u_factors[u_idx].T, i_factors[i_idx]))\n",
    "    return error/X.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AE8UpWc1Sn9u"
   },
   "source": [
    "### Random Predictor Error\n",
    "\n",
    "**Exercise**: What is the expected error of a random predictor given that the actual ratings are uniformly distributed between 1 and 5?\n",
    "\n",
    "Below is a function for calculating this error experimentally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NX4-hb8vSn9v"
   },
   "outputs": [],
   "source": [
    "def random_predictor_error(X):\n",
    "    error = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        u_idx = X.iloc[i,0]\n",
    "        i_idx = X.iloc[i,1]\n",
    "        error += np.abs(X.iloc[i,2] - np.random.randint(1,6))\n",
    "    return error/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bvgyC1VJSn9v"
   },
   "outputs": [],
   "source": [
    "# initialize factor matrices\n",
    "item_factors = {}\n",
    "user_factors = {}\n",
    "for r in range(n_prefs):\n",
    "    user_factors[prefs.iloc[r,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "    item_factors[prefs.iloc[r,1]] = np.random.rand(n_factors,1) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1663511703257,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "tN1NtOQISn9w",
    "outputId": "3187371e-4586-4f1d-fcf4-79cfd0388a3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24385625],\n",
       "       [-0.08176695]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23479,
     "status": "ok",
     "timestamp": 1663511728591,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "qAVXvpFuSn9w",
    "outputId": "65041189-2e4f-4aad-c3f4-408684a7c4a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random predictor error:  1.4874846285056924\n"
     ]
    }
   ],
   "source": [
    "print(\"Random predictor error: \", random_predictor_error(prefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhCmDB3TSn9x"
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Following is the stochastic gradient algorithm which is popularized by [Simon Funk](https://sifter.org/simon/journal/20061211.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "executionInfo": {
     "elapsed": 88951,
     "status": "error",
     "timestamp": 1663511961996,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "_km686P-Sn9x",
    "outputId": "f1214f6d-aaad-4ef7-9c67-0b4b4f4a742c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error:  [[3.50413338]]\n",
      "Iteration  0\n",
      "Train error:  [[1.04465786]]\n",
      "Test error:  [[1.13842265]]\n",
      "Iteration  1\n",
      "Train error:  [[0.81035163]]\n",
      "Test error:  [[0.92475216]]\n",
      "Iteration  2\n",
      "Train error:  [[0.73909106]]\n",
      "Test error:  [[0.86588175]]\n",
      "Iteration  3\n",
      "Train error:  [[0.7094839]]\n",
      "Test error:  [[0.84976741]]\n",
      "Iteration  4\n",
      "Train error:  [[0.69031002]]\n",
      "Test error:  [[0.83458268]]\n",
      "Iteration  5\n",
      "Train error:  [[0.67585334]]\n",
      "Test error:  [[0.82347845]]\n",
      "Iteration  6\n",
      "Train error:  [[0.65990782]]\n",
      "Test error:  [[0.8116561]]\n",
      "Iteration  7\n",
      "Train error:  [[0.65234064]]\n",
      "Test error:  [[0.81485354]]\n",
      "Iteration  8\n",
      "Train error:  [[0.64748465]]\n",
      "Test error:  [[0.80862325]]\n",
      "Iteration  9\n",
      "Train error:  [[0.63391101]]\n",
      "Test error:  [[0.8051957]]\n"
     ]
    }
   ],
   "source": [
    "item_factors = {}\n",
    "user_factors = {}\n",
    "for r in range(n_prefs):\n",
    "    user_factors[prefs.iloc[r,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "    item_factors[prefs.iloc[r,1]] = np.random.rand(n_factors,1) - 0.5\n",
    "    \n",
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "\n",
    "# Stochastic Gradient descent\n",
    "alpha = 0.03\n",
    "my_lambda = 0.1\n",
    "n_iters = 10\n",
    "    \n",
    "print(\"Initial error: \", calc_error(X_train, user_factors, item_factors))\n",
    "\n",
    "for t in range(n_iters):\n",
    "    #q = 10\n",
    "    #ux = prefs.iloc[q,0]; ix = prefs.iloc[q,1]\n",
    "    #print(\"user factor: \",user_factors[ux],\"item factor: \", item_factors[ix])\n",
    "    #print(\"actual rating: \", prefs.iloc[q,2], \"predicted rating: \", np.dot(user_factors[ux].T,item_factors[ix]))\n",
    "    X_train = shuffle(X_train)\n",
    "    for r in range(X_train.shape[0]):\n",
    "        u = X_train.iloc[r,0]\n",
    "        i = X_train.iloc[r,1]\n",
    "        error = X_train.iloc[r,2] - np.dot(user_factors[u].T, item_factors[i])[0,0]\n",
    "        user_factors[u] = user_factors[u] + alpha*(error*item_factors[i] - my_lambda*user_factors[u])\n",
    "        item_factors[i] = item_factors[i] + alpha*(error*user_factors[u] - my_lambda*item_factors[i])  \n",
    "       \n",
    "          \n",
    "    print(\"Iteration \", t)\n",
    "    print(\"Train error: \", calc_error(X_train, user_factors, item_factors))\n",
    "    print(\"Test error: \", calc_error(X_test, user_factors, item_factors))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TeaazvPFSn9x",
    "outputId": "dd33ecd6-a3f8-4282-80c9-ed82282505a9"
   },
   "outputs": [],
   "source": [
    "item_factors = {}\n",
    "user_factors = {}\n",
    "for r in range(n_prefs):\n",
    "    user_factors[prefs.iloc[r,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "    item_factors[prefs.iloc[r,1]] = np.random.rand(n_factors,1) - 0.5\n",
    "    \n",
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "\n",
    "# Stochastic Gradient descent\n",
    "alpha = 0.03\n",
    "my_lambda = 0.1\n",
    "n_iters = 10\n",
    "    \n",
    "print(\"Initial error: \", calc_error(X_train, user_factors, item_factors))\n",
    "\n",
    "for t in range(n_iters):\n",
    "    #q = 10\n",
    "    #ux = prefs.iloc[q,0]; ix = prefs.iloc[q,1]\n",
    "    #print(\"user factor: \",user_factors[ux],\"item factor: \", item_factors[ix])\n",
    "    #print(\"actual rating: \", prefs.iloc[q,2], \"predicted rating: \", np.dot(user_factors[ux].T,item_factors[ix]))\n",
    "    X_train = shuffle(X_train)\n",
    "    for r in range(X_train.shape[0]):\n",
    "        u = X_train.iloc[r,0]\n",
    "        i = X_train.iloc[r,1]\n",
    "        error = X_train.iloc[r,2] - np.dot(user_factors[u].T, item_factors[i])[0,0]\n",
    "        user_factors[u] = user_factors[u] + alpha*(error*item_factors[i] - my_lambda*user_factors[u])\n",
    "        item_factors[i] = item_factors[i] + alpha*(error*user_factors[u] - my_lambda*item_factors[i])  \n",
    "       \n",
    "          \n",
    "    print(\"Iteration \", t)\n",
    "    print(\"Train error: \", calc_error(X_train, user_factors, item_factors))\n",
    "    print(\"Test error: \", calc_error(X_test, user_factors, item_factors))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgGw4j4GSn9y"
   },
   "source": [
    "### Batch Gradient Descent\n",
    "If you run the code below you will see that both training and test errors decrease very slowly. Eventually there will be convergence but compared to stochastic version it will be very slow. It is a good example to show the speed advantage of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "executionInfo": {
     "elapsed": 150922,
     "status": "error",
     "timestamp": 1663512327284,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "MdUJbtKBSn9y",
    "outputId": "30deb2e4-31ee-4bbe-889e-023e6b47aa3d"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import copy\n",
    "#initialize factor matrices\n",
    "item_factors = {}\n",
    "user_factors = {}\n",
    "for i in range(n_prefs):\n",
    "    user_factors[prefs.iloc[i,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "    item_factors[prefs.iloc[i,1]] = np.random.rand(n_factors,1) - 0.5\n",
    "    \n",
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "\n",
    "train_users = X_train.iloc[:,0].unique()\n",
    "train_items = X_train.iloc[:,1].unique()\n",
    "R = csr_matrix((X_train.iloc[:,2], (X_train.iloc[:,0],X_train.iloc[:,1])))\n",
    "\n",
    "# Batch Gradient descent\n",
    "alpha = 0.1\n",
    "my_lambda = 0.1\n",
    "n_iters = 100\n",
    "\n",
    "for t in range(n_iters):\n",
    "    for u in train_users:\n",
    "        I_u = X_train[X_train.userId==u].iloc[:,1]\n",
    "        sum_total = 0\n",
    "        for i in I_u:\n",
    "            sum_total += (R[u,i] - np.dot(item_factors[i].T, user_factors[u])[0,0])*item_factors[i]\n",
    "        sum_total = sum_total / I_u.size\n",
    "        user_factors[u] = user_factors[u] + alpha*(sum_total - my_lambda*user_factors[u])\n",
    "    for i in train_items:\n",
    "        U_i = X_train[X_train.movieId==i].iloc[:,0]\n",
    "        sum_total = 0\n",
    "        for u in U_i:\n",
    "            sum_total += (R[u,i] - np.dot(item_factors[i].T, user_factors[u])[0,0])*user_factors[u]\n",
    "        sum_total = sum_total / U_i.size\n",
    "        item_factors[i] = item_factors[i] + alpha*(sum_total - my_lambda*item_factors[i])\n",
    "        \n",
    "    print(\"Iteration \", t)\n",
    "    print(\"Train error: \", calc_error(X_train,user_factors,item_factors))\n",
    "    print(\"Test error: \", calc_error(X_test,user_factors,item_factors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcgyM8a9Sn9y"
   },
   "source": [
    "### Question\n",
    "\n",
    "Suppose that user A appears in 200 rows in the user-item preferences dataset. In a single iteration how many updates will there be to the latent vector $p_u$ in stochastic GD vd Batch GD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOwthAvMSn9y"
   },
   "source": [
    "### Alternating Least Squares (ALS)\n",
    "Since the datasets in this domain are really large, people always try to find ways to speed up the optimization processes. One popular algorithm is called ALS. Here the basic idea is that although the cost function is not convex, when either user factors or items factors are fixed then it becomes a convex function which can be directly solved. The algorithm alternates between updating user and item factors. ([Large-scale parallel collaborative filtering for the netflix prize](https://link.springer.com/chapter/10.1007/978-3-540-68880-8_32)). \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial p_{ku}}=-\\sum_{i \\in I_u}2(r_{ui} - q^T_ip_u)q_{ki} + 2\\lambda p_{ku} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i \\in I_u} q_{ki}q^T_ip_u + \\lambda p_{ku} = \\sum_{i \\in I_u}q_{ki}r_{ui} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i \\in I_u} q_{i}q^T_ip_u + \\lambda p_{u} = \\sum_{i \\in I_u}q_{i}r_{ui} \n",
    "$$\n",
    "\n",
    "$$\n",
    "(Q_{I_u}Q_{I_u}^T + \\lambda I) p_{u} = Q_{I_u}R^T(u,I_u)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{u} = (Q_{I_u}Q_{I_u}^T + \\lambda I)^{-1}Q_{I_u}R^T(u,I_u)\n",
    "$$\n",
    "\n",
    "where $I$ is the $f × f$ identity matrix. $Q_{I_u}$ denotes the sub-matrix of $Q$ where columns $j \\in I_u$ are selected, and $R^T(u,I_u)$ is the row vector where columns $j \\in I_u$ of the $u$-th row of $R$ are selected.\n",
    "\n",
    "Similarly for $q_i$ we have:\n",
    "\n",
    "$$\n",
    "q_{i} = (P_{U_i}P_{U_i}^T + \\lambda I)^{-1}P_{U_i}R(U_i,i)\n",
    "$$\n",
    "\n",
    "where $I$ is the $f × f$ identity matrix. $P_{U_i}$ denotes the sub-matrix of $P$ where columns $j \\in U_i$ are selected, and $R(U_i,i)$ is the column vector where columns $j \\in U_i$ of the $i$-th column of $R$ are selected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqRNL5UASn9z",
    "outputId": "82ea954e-aef7-4d03-fa8c-88e53f9f9a1d"
   },
   "outputs": [],
   "source": [
    "#initialize factor matrices\n",
    "Q = pd.DataFrame(np.random.rand(n_factors,n_items)-0.5, columns=items)\n",
    "P = pd.DataFrame(np.random.rand(n_factors,n_users)-0.5, columns=users)\n",
    "\n",
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "\n",
    "train_users = X_train.iloc[:,0].unique()\n",
    "train_items = X_train.iloc[:,1].unique()\n",
    "R = csr_matrix((X_train.iloc[:,2], (X_train.iloc[:,0],X_train.iloc[:,1])))\n",
    "\n",
    "alpha = 0.030\n",
    "my_lambda = 0.1\n",
    "n_iters = 100\n",
    "\n",
    "for t in range(n_iters):\n",
    "    for u in train_users:\n",
    "        I_u = X_train[X_train.userId==u].iloc[:,1]\n",
    "        A = np.dot(Q[I_u],Q[I_u].T)+my_lambda*np.identity(n_factors)\n",
    "        V = np.dot(Q[I_u],R[u,I_u].todense().T)\n",
    "        P[u] = np.dot(np.linalg.inv(A),V)\n",
    "    for i in train_items:\n",
    "        U_i = X_train[X_train.movieId==i].iloc[:,0]\n",
    "        A = np.dot(P[U_i],P[U_i].T)+my_lambda*np.identity(n_factors)\n",
    "        V = np.dot(P[U_i],R[U_i,i].todense())     \n",
    "        Q[i] = np.dot(np.linalg.inv(A),V)\n",
    "        \n",
    "    print(\"Iteration \", t)\n",
    "    print(\"Train error: \", calc_error(X_train,P,Q))\n",
    "    print(\"Test error: \", calc_error(X_test,P,Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7syLd1bcSn9z",
    "outputId": "8857fc99-aaec-4679-e30b-ed45646928c2"
   },
   "outputs": [],
   "source": [
    "A  = X_train[X_train.userId==1].iloc[:,1]\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfgSAsg7Sn9z",
    "outputId": "ade4a33e-0bfa-4ea8-c951-c0882ee8d766"
   },
   "outputs": [],
   "source": [
    "R[1,A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXgt5udkSn90"
   },
   "outputs": [],
   "source": [
    "class MF:\n",
    "    \"\"\"\n",
    "    prefs: matrix of prefences, column0=userid, column1=itemid, column2=pref, column3=timestamp \n",
    "    \"\"\"\n",
    "    def __init__(self, prefs, alpha=0.03, mylambda=0.1, n_factors = 10, n_iters = 50):\n",
    "        self.alpha = alpha\n",
    "        self.mylambda = mylambda\n",
    "        self.n_iters = n_iters\n",
    "        self.item_factors = {}\n",
    "        self.user_factors = {}\n",
    "        self.prefs = prefs\n",
    "        # prefs is a matrix containing u, i, r values in each row. This is useful to shuffle and pass over\n",
    "        # the data multiple times in an efficient way in the fit() method.\n",
    "        for r in range(self.prefs.shape[0]):\n",
    "            self.user_factors[self.prefs[r,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "            self.item_factors[self.prefs[r,1]] = np.random.rand(n_factors,1) - 0.5\n",
    "        print(\"Finished initialization\")\n",
    "        \n",
    "     \n",
    "    def calc_error(self, X):\n",
    "        error = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            u_idx = X[i,0]\n",
    "            i_idx = X[i,1]\n",
    "            error += np.abs(X[i,2] - np.dot(self.user_factors[u_idx].T, self.item_factors[i_idx]))\n",
    "        print(\"Random Predictor Error:\",error/self.prefs.shape[0])\n",
    "        \n",
    "    def fit(self, verbose=False, method=\"SGD\"):\n",
    "        if (method == \"Random\"):\n",
    "            error = 0\n",
    "            for i in range(prefs.shape[0]):\n",
    "                u_idx = prefs[i,0]\n",
    "                i_idx = prefs[i,1]\n",
    "                error += np.abs(prefs[i,2] - np.random.randint(1,6))\n",
    "            return error/prefs.shape[0]\n",
    "            \n",
    "        elif (method == \"SGD\"):\n",
    "            if (verbose): \n",
    "                print(\"Initial error: \", self.calc_error(prefs))                      \n",
    "            for t in range(self.n_iters):\n",
    "                self.prefs = shuffle(self.prefs)\n",
    "                for r in range(self.prefs.shape[0]):\n",
    "                    u = self.prefs[r,0]\n",
    "                    i = self.prefs[r,1]\n",
    "                    error = self.prefs[r,2] - np.dot(self.user_factors[u].T, self.item_factors[i])[0,0]\n",
    "                    self.user_factors[u] = self.user_factors[u] + self.alpha*(error*self.item_factors[i] - self.mylambda*self.user_factors[u])\n",
    "                    self.item_factors[i] = self.item_factors[i] + self.alpha*(error*self.user_factors[u] - self.mylambda*self.item_factors[i])  \n",
    "            \n",
    "                if (verbose): \n",
    "                    print(\"Iteration: \", t)\n",
    "                if (verbose): \n",
    "                    print(\"Train error: \", self.calc_error(self.prefs))                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1345,
     "status": "ok",
     "timestamp": 1663506026832,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "nHyb23UT0QZ5",
    "outputId": "1f7cb3cb-7269-4086-f925-abe536a95492"
   },
   "outputs": [],
   "source": [
    "mf = MF(prefs)\n",
    "mf.fit(verbose=True, method=\"Random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "executionInfo": {
     "elapsed": 1741,
     "status": "error",
     "timestamp": 1663506181842,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "Jk47h4VO05Wx",
    "outputId": "383a5b07-db86-4d3b-a3f9-004309530965"
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "mf = MF(X_train, n_iters=3)\n",
    "mf.fit(verbose=True, method=\"SGD\")\n",
    "print(\"Test error: \", mf.calc_error(X_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1663506254127,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "329s7qtq3ZQS",
    "outputId": "a7d57cf1-3a9f-4348-ddb7-ea338b6846bc"
   },
   "outputs": [],
   "source": [
    "prefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuBrKXzppkp4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
