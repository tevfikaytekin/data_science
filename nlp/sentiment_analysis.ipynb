{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4hIJ7XzCqW-L"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn import metrics\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, classification_report, confusion_matrix\n","from sklearn.linear_model import LogisticRegression, LinearRegression\n","from lightgbm import LGBMClassifier, LGBMRegressor\n","from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_Pq8oWowGW5"},"outputs":[],"source":["nltk.download('punkt')\n","nltk.download('punkt_tab')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLIxeMzIueEt"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDhiSRF7qW-M"},"outputs":[],"source":["# Read in data\n","#df = pd.read_csv('../../datasets/amazon-fine-food-reviews/Reviews.csv')\n","df = pd.read_csv('/content/drive/MyDrive/datasets/amazon-fine-food-reviews/Reviews.csv')\n","print(df.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMd8qxljqW-N"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"gBL3uZiuqW-N"},"source":["Distribution of Scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YPMsbdvqW-O"},"outputs":[],"source":["plt.figure(figsize=(4,3))\n","\n","ax = df['Score'].value_counts().sort_index() \\\n","    .plot(kind='bar',\n","          title='Count of Reviews by Stars')\n","ax.set_xlabel('Review Stars')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wd5W4vwkqW-O"},"source":["Look at an example text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZKhB3B6qW-P"},"outputs":[],"source":["example = df['Text'][50]\n","print(example)"]},{"cell_type":"markdown","metadata":{"id":"kV0kb8scNzx-"},"source":["Distribution of text length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cCVMGg6XNzx-"},"outputs":[],"source":["lengths = [len(text) for text in df.Text]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxF6sdyhNzx-"},"outputs":[],"source":["plt.hist(lengths, bins=100);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLyURgs-Nzx-"},"outputs":[],"source":["lengths = [item for item in lengths if item < 3000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i26E3XzMNzx_"},"outputs":[],"source":["plt.hist(lengths, bins=100);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfyqkw4qqW-P"},"outputs":[],"source":["tokens = nltk.word_tokenize(example)\n","tokens[:10]"]},{"cell_type":"markdown","metadata":{"id":"Bp9aXwRcqW-Q"},"source":["### Sentiment Analysis with NLTK Vader\n","\n","NLTK Vader is a simple rule based sentiment analysis model. More information can be found at:\n","\n","Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for\n","Sentiment Analysis of Social Media Text. Eighth International Conference on\n","Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqLNPt4rwQTB"},"outputs":[],"source":["nltk.download('vader_lexicon')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ueATc1D2qW-Q"},"outputs":[],"source":["from nltk.sentiment import SentimentIntensityAnalyzer\n","from tqdm.notebook import tqdm\n","\n","sia = SentimentIntensityAnalyzer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ru4KWim7qW-R"},"outputs":[],"source":["sia.polarity_scores('I am so happy!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r7tgln9GqW-R"},"outputs":[],"source":["sia.polarity_scores('This is the worst thing ever.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wtVDhVChNzyA"},"outputs":[],"source":["sia.polarity_scores('This is not a good movie.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IF8fHxu_qW-R"},"outputs":[],"source":["sia.polarity_scores(example)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54ADk16aqW-R"},"outputs":[],"source":["from collections import Counter\n","#df_small = df.sample(n=10000)\n","df_small = df[:10000].copy()\n","df_small['Score'] = df['Score'].apply(lambda x: 1 if x > 3 else 0)\n","Counter(df_small.Score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHN2dcheqW-R"},"outputs":[],"source":["# Run the polarity score on the entire dataset\n","res = {}\n","for i, row in tqdm(df_small.iterrows(), total=len(df_small)):\n","    text = row['Text']\n","    myid = row['Id']\n","    res[myid] = sia.polarity_scores(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfHtmYROqW-R"},"outputs":[],"source":["vaders = pd.DataFrame(res).T\n","vaders = vaders.reset_index().rename(columns={'index': 'Id'})\n","vaders = vaders.merge(df_small, how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"obcqUrgTqW-S"},"outputs":[],"source":["# Now we have sentiment score and metadata\n","vaders.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqRGyQF9qW-S"},"outputs":[],"source":["print(np.max(vaders.compound))\n","print(np.min(vaders.compound))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M11TwqjhqW-S"},"outputs":[],"source":["print(np.max(vaders.Score))\n","print(np.min(vaders.Score))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZvPZELLqW-S"},"outputs":[],"source":["def scale_score(x):\n","    return (x+1)/2\n","\n","scaled_scores = [scale_score(x) for x in vaders.compound]\n","print(scaled_scores)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDZ3bhROqW-S"},"outputs":[],"source":["print(\"MAE:\", mean_absolute_error(vaders.Score, scaled_scores))\n","print(\"MSE:\", mean_squared_error(vaders.Score, scaled_scores))"]},{"cell_type":"markdown","metadata":{"id":"LxjXrwu4qW-T"},"source":["For comparison, following shows the performance of random guess"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysrdgB4RqW-T"},"outputs":[],"source":["random_scores = np.random.uniform(0, 1, len(vaders))\n","print(\"MAE random:\", mean_absolute_error(vaders.Score,random_scores))\n","print(\"MSE random:\", mean_squared_error(vaders.Score, random_scores))\n","\n","random_scores = np.random.randint(0, 2, len(vaders))\n","print(classification_report(vaders.Score,random_scores))\n","print(confusion_matrix(vaders.Score,random_scores))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4Oxnfj_NzyC"},"outputs":[],"source":["Counter(vaders.Score)"]},{"cell_type":"markdown","metadata":{"id":"tlKg48VSqW-T"},"source":["### Sentiment Analysis with machine learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cKFDQIAeNzyC"},"outputs":[],"source":["#df_small = df.sample(n=10000)\n","df_small = df[:10000].copy()\n","df_small['Score'] = df['Score'].apply(lambda x: 1 if x > 3 else 0)\n","Counter(df_small.Score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dH6FWx8qW-T"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Create TF-IDF features\n","tfidf_vectorizer = TfidfVectorizer(max_features=100, max_df=0.1)\n","X = tfidf_vectorizer.fit_transform(df_small['Text'])\n","y = df_small['Score'].values"]},{"cell_type":"code","source":["tfidf_vectorizer.get_feature_names_out()"],"metadata":{"id":"pWHxrGdgQtvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GD_WalIGNzyC"},"outputs":[],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ZnIY2UZqW-T"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n","\n","model1 = LGBMClassifier()\n","model2 = LGBMRegressor()\n","model1.fit(X_train, y_train)\n","model2.fit(X_train, y_train)\n","\n","# Predict and evaluate the classifier\n","predictions1 = model1.predict(X_test)\n","predictions2 = model2.predict(X_test)\n","\n","print(\"MAE classifier:\", mean_absolute_error(y_test, predictions1))\n","print(\"MSE classifier:\", mean_squared_error(y_test, predictions1))\n","\n","print(classification_report(y_test, predictions1))\n","print(confusion_matrix(y_test, predictions1))\n","\n","print(\"MAE regressor:\", mean_absolute_error(y_test, predictions2))\n","print(\"MSE regressor:\", mean_squared_error(y_test, predictions2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZGYmRKbNzyD"},"outputs":[],"source":["Counter(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyrsK117NzyO"},"outputs":[],"source":["np.random.randint(-1,2)"]},{"cell_type":"markdown","metadata":{"id":"MfmuASW9qW-T"},"source":["Results are better than the rule based sentiment analysis. Note that, we did not use the entire dataset, more data will improve the performance of the ML classifiers. Aslo note that this is a simple TFIDF+ML implementation, there is room for improvement. Another thing to note is that modeling the problem as a classification problem (as opposed to a regression problem) gave better results in terms of MAE but worse results in terms of MSE. This is reasonable since for the classifier predicting a wrong class amounts to the same error regardless of the actaul value, so the classifier optimizes for predicting the correct class rather than predicting a class which is numerically closer. On the other hand the regressor tries to minimize the distance between the actual value and the prediction. Hence we get lower MAE but higher MSE using a classifier."]},{"cell_type":"markdown","metadata":{"id":"6_4LY7AyNzyO"},"source":["### Word Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rd5qRTdPNzyO"},"outputs":[],"source":["import sys\n","!{sys.executable} -m pip install gensim\n","from gensim.models import Word2Vec\n","from gensim.models.word2vec import LineSentence\n","from sklearn.metrics.pairwise import cosine_similarity\n","import gensim.downloader\n","from nltk.tokenize import RegexpTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFif9xoyNzyP"},"outputs":[],"source":["embeddings = gensim.downloader.load('glove-twitter-25')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Bjm-lZPNzyP"},"outputs":[],"source":["import re\n","\n","def simple_preprocess(text):\n","    text = text.lower()  # convert text to lower-case\n","    text = re.sub(r\"@\\w+\", \"\", text)  # remove mentions\n","    text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n","    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text, re.I|re.A)  # remove non-letters\n","    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove excess whitespace\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XD3XlZwNzyP"},"outputs":[],"source":["def text_to_vector(text, embeddings):\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","\n","    tokens = simple_preprocess(text)\n","\n","    vectors = []\n","\n","    for token in tokenizer.tokenize(tokens):\n","        # Check if the token is in the embeddings vocabulary\n","        if token in embeddings:\n","            vectors.append(embeddings[token])\n","\n","    # If we found any vectors, calculate the mean, otherwise return a zero vector\n","    if vectors:\n","        return np.mean(vectors, axis=0)\n","    else:\n","        return np.zeros(embeddings.vector_size)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oKTJtvrFNzyP"},"outputs":[],"source":["# Convert texts to vectors\n","vectorized_texts = df_small['Text'].apply(lambda x: text_to_vector(x, embeddings))\n","\n","# Convert to numpy array\n","X = np.array(vectorized_texts.tolist())\n","y = df_small['Score'].values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzduSKuBNzyP"},"outputs":[],"source":["print(X.shape)\n","X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"37gz7JSxNzyP"},"outputs":[],"source":["# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n","\n","model1 = LGBMClassifier()\n","model2 = LGBMRegressor()\n","model1.fit(X_train, y_train)\n","model2.fit(X_train, y_train)\n","\n","# Predict and evaluate the classifier\n","predictions1 = model1.predict(X_test)\n","predictions2 = model2.predict(X_test)\n","\n","print(\"MAE classifier:\", mean_absolute_error(y_test, predictions1))\n","print(\"MSE classifier:\", mean_squared_error(y_test, predictions1))\n","\n","print(classification_report(y_test, predictions1))\n","print(confusion_matrix(y_test, predictions1))\n","\n","print(\"MAE regressor:\", mean_absolute_error(y_test, predictions2))\n","print(\"MSE regressor:\", mean_squared_error(y_test, predictions2))\n"]},{"cell_type":"markdown","metadata":{"id":"4gg1LurDNzyQ"},"source":["### Doc2Vec\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eOYlh1pFNzyQ"},"outputs":[],"source":["texts = df_small['Text'].tolist()\n","tokenized_texts = [word_tokenize(text.lower()) for text in texts]"]},{"cell_type":"code","source":["tokenized_texts[0]"],"metadata":{"id":"atQRU8SRTFQl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vEfho8TUNzyQ"},"outputs":[],"source":["from gensim.models.doc2vec import Doc2Vec, TaggedDocument"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzSHfwpuNzyQ"},"outputs":[],"source":["documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized_texts)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FznQZoVPNzyQ"},"outputs":[],"source":["documents[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPvm0XkXNzyQ"},"outputs":[],"source":["model = Doc2Vec(documents, epochs=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIC2vI86NzyQ"},"outputs":[],"source":["# Convert texts to vectors\n","vectorized_texts = df_small['Text'].apply(lambda x: model.infer_vector(word_tokenize(x.lower())))\n","\n","# Convert to numpy array\n","X = np.array(vectorized_texts.tolist())\n","y = df_small['Score'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7D9-N4PWNzyR"},"outputs":[],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUuL_mSdNzyR"},"outputs":[],"source":["# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n","\n","model1 = LGBMClassifier()\n","model2 = LGBMRegressor()\n","model1.fit(X_train, y_train)\n","model2.fit(X_train, y_train)\n","\n","# Predict and evaluate the classifier\n","predictions1 = model1.predict(X_test)\n","predictions2 = model2.predict(X_test)\n","\n","print(\"MAE classifier:\", mean_absolute_error(y_test, predictions1))\n","print(\"MSE classifier:\", mean_squared_error(y_test, predictions1))\n","\n","print(classification_report(y_test, predictions1))\n","print(confusion_matrix(y_test, predictions1))\n","\n","print(\"MAE regressor:\", mean_absolute_error(y_test, predictions2))\n","print(\"MSE regressor:\", mean_squared_error(y_test, predictions2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHLXBzrFNzyR"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}