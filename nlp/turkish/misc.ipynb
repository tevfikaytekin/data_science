{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various examples featuring text in Turkish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['köpek', 'koşuyor', 'gül', 'yazdık', 'yazdıklarımz']\n"
     ]
    }
   ],
   "source": [
    "from snowballstemmer import TurkishStemmer\n",
    "\n",
    "words = [\"köpekler\", \"koşuyordum\", \"güller\", \"yazdıklarımızdan\", \"yazdıklarımzdan\"]\n",
    "\n",
    "turkStem = TurkishStemmer()\n",
    "stems = [turkStem.stemWord(word) for word in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilgisayar, aritmetik veya mantıksal işlem dizilerini (berim) otomatik olarak yürütmek üzere programlanabilen dijital bir elektronik makinedir.\n",
      "Çağdaş bilgisayarlar, programlar olarak bilinen genel işlem kümelerini gerçekleştirebilir.\n",
      "Bu programlar, bilgisayarların çeşitli görevleri gerçekleştirmesini sağlar.\n",
      "Ayrıca bir bilgisayar sisteminin tam verimle çalışabilmesi için donanım, işletim sistemi ve çevresel cihazlara sahip olması gerekmektedir.\n",
      "Bu terim aynı zamanda bir bilgisayar ağı veya bilgisayar kümesi gibi birbirine bağlı ve birlikte çalışan bir grup bilgisayar anlamına da gelebilir.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Bilgisayar, aritmetik veya mantıksal işlem dizilerini (berim) \\\n",
    "otomatik olarak yürütmek üzere programlanabilen dijital bir elektronik makinedir.  \\\n",
    "Çağdaş bilgisayarlar, programlar olarak bilinen genel işlem kümelerini gerçekleştirebilir. \\\n",
    "Bu programlar, bilgisayarların çeşitli görevleri gerçekleştirmesini sağlar. \\\n",
    "Ayrıca bir bilgisayar sisteminin tam verimle çalışabilmesi için donanım, \\\n",
    "işletim sistemi ve çevresel cihazlara sahip olması gerekmektedir. Bu terim aynı \\\n",
    "zamanda bir bilgisayar ağı veya bilgisayar kümesi gibi birbirine bağlı ve birlikte \\\n",
    "çalışan bir grup bilgisayar anlamına da gelebilir.\"\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "for sent in sentences:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bilgisayar', ',', 'aritmetik', 'veya', 'mantıksal', 'işlem', 'dizilerini', '(', 'berim', ')', 'otomatik', 'olarak', 'yürütmek', 'üzere', 'programlanabilen', 'dijital', 'bir', 'elektronik', 'makinedir', '.', 'Çağdaş', 'bilgisayarlar', ',', 'programlar', 'olarak', 'bilinen', 'genel', 'işlem', 'kümelerini', 'gerçekleştirebilir', '.', 'Bu', 'programlar', ',', 'bilgisayarların', 'çeşitli', 'görevleri', 'gerçekleştirmesini', 'sağlar', '.', 'Ayrıca', 'bir', 'bilgisayar', 'sisteminin', 'tam', 'verimle', 'çalışabilmesi', 'için', 'donanım', ',', 'işletim', 'sistemi', 've', 'çevresel', 'cihazlara', 'sahip', 'olması', 'gerekmektedir', '.', 'Bu', 'terim', 'aynı', 'zamanda', 'bir', 'bilgisayar', 'ağı', 'veya', 'bilgisayar', 'kümesi', 'gibi', 'birbirine', 'bağlı', 've', 'birlikte', 'çalışan', 'bir', 'grup', 'anlamına', 'da', 'gelebilir', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello! Welcome to the world of Natural Language Processing with NLTK.\"\n",
    "\n",
    "\n",
    "text = \"Bilgisayar, aritmetik veya mantıksal işlem dizilerini (berim) \\\n",
    "otomatik olarak yürütmek üzere programlanabilen dijital bir elektronik makinedir.  \\\n",
    "Çağdaş bilgisayarlar, programlar olarak bilinen genel işlem kümelerini gerçekleştirebilir. \\\n",
    "Bu programlar, bilgisayarların çeşitli görevleri gerçekleştirmesini sağlar. \\\n",
    "Ayrıca bir bilgisayar sisteminin tam verimle çalışabilmesi için donanım, \\\n",
    "işletim sistemi ve çevresel cihazlara sahip olması gerekmektedir. Bu terim aynı \\\n",
    "zamanda bir bilgisayar ağı veya bilgisayar kümesi gibi birbirine bağlı ve birlikte \\\n",
    "çalışan bir grup anlamına da gelebilir.\"\n",
    "\n",
    "tokens = word_tokenize(text, language=\"turkish\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APPENDING RESULT: <(gelmek_Verb)(-)(gel:verbRoot_S + mekte:vProgMakta_S + vA3sg_ST + dir:vCop_ST)>\n",
      "APPENDING RESULT: <(gelmek_Verb)(-)(gel:verbRoot_S + mek:vInf1_S + nounInf1Root_S + a3sgInf1_S + pnonInf1_S + te:loc_ST + nounZeroDeriv_S + nVerb_S + nPresent_S + nA3sg_S + dir:nCop_ST)>\n",
      "APPENDING RESULT: <(yazmak_Verb)(-)(yaz:verbRoot_S + mış:vNarr_S + lar:vA3pl_ST)>\n",
      "APPENDING RESULT: <(yazmak_Verb)(-)(yaz:verbRoot_S + mış:vNarrPart_S + adjectiveRoot_ST + adjZeroDeriv_S + nVerb_S + nPresent_S + lar:nA3pl_ST)>\n",
      "APPENDING RESULT: <(yazmak_Verb)(-)(yaz:verbRoot_S + mış:vNarrPart_S + adjectiveRoot_ST + adjZeroDeriv_S + noun_S + lar:a3pl_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(yaz_Noun_Time)(-)(yaz:noun_S + a3sg_S + pnon_S + nom_ST + nounZeroDeriv_S + nVerb_S + mış:nNarr_S + lar:nA3pl_ST)>\n",
      "APPENDING RESULT: <(olmak_Verb)(-)(ol:verbRoot_S + abil:vAble_S + verbRoot_S + dik:vPastPart_S + noun_S + ler:a3pl_S + imiz:p1pl_S + nom_ST)>\n",
      "APPENDING RESULT: <(okumak_Verb)(-)(oku:verbRoot_S + muş:vNarrPart_S + adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(okumak_Verb)(-)(oku:verbRoot_S + muş:vNarr_S + vA3sg_ST)>\n",
      "APPENDING RESULT: <(gülmek_Verb)(-)(gül:verbRoot_S + mek:vInf1_S + nounInf1Root_S + a3sgInf1_S + pnonInf1_S + nom_ST)>\n",
      "APPENDING RESULT: <(olta_Noun)(-)(olta:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ağız_Noun)(-)(ağz:noun_S + a3sg_S + pnon_S + ı:acc_ST)>\n",
      "APPENDING RESULT: <(ağız_Noun)(-)(ağz:noun_S + a3sg_S + ı:p3sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(had_Noun)(-)(hadd:noun_S + a3sg_S + pnon_S + i:acc_ST)>\n",
      "APPENDING RESULT: <(had_Noun)(-)(hadd:noun_S + a3sg_S + i:p3sg_S + nom_ST)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: gelmektedir\n",
      "Analysis: [Parse(word='gelmektedir', lemma='gelmek', pos='Verb', morphemes=['Verb', 'Prog2', 'A3sg', 'Cop'], formatted='[gelmek:Verb] gel:Verb+mekte:Prog2+A3sg+dir:Cop'), Parse(word='gelmektedir', lemma='gelmek', pos='Verb', morphemes=['Verb', 'Inf1', 'Noun', 'A3sg', 'Loc', 'Zero', 'Verb', 'Pres', 'A3sg', 'Cop'], formatted='[gelmek:Verb] gel:Verb|mek:Inf1→Noun+A3sg+te:Loc|Zero→Verb+Pres+A3sg+dir:Cop')]\n",
      "------------------------------\n",
      "Word: yazmışlar\n",
      "Analysis: [Parse(word='yazmışlar', lemma='yazmak', pos='Verb', morphemes=['Verb', 'Narr', 'A3pl'], formatted='[yazmak:Verb] yaz:Verb+mış:Narr+lar:A3pl'), Parse(word='yazmışlar', lemma='yazmak', pos='Verb', morphemes=['Verb', 'NarrPart', 'Adj', 'Zero', 'Verb', 'Pres', 'A3pl'], formatted='[yazmak:Verb] yaz:Verb|mış:NarrPart→Adj|Zero→Verb+Pres+lar:A3pl'), Parse(word='yazmışlar', lemma='yazmak', pos='Noun', morphemes=['Verb', 'NarrPart', 'Adj', 'Zero', 'Noun', 'A3pl'], formatted='[yazmak:Verb] yaz:Verb|mış:NarrPart→Adj|Zero→Noun+lar:A3pl'), Parse(word='yazmışlar', lemma='yaz', pos='Verb', morphemes=['Noun', 'A3sg', 'Zero', 'Verb', 'Narr', 'A3pl'], formatted='[yaz:Noun,Time] yaz:Noun+A3sg|Zero→Verb+mış:Narr+lar:A3pl')]\n",
      "------------------------------\n",
      "Word: olabildiklerimiz\n",
      "Analysis: [Parse(word='olabildiklerimiz', lemma='olmak', pos='Noun', morphemes=['Verb', 'Able', 'Verb', 'PastPart', 'Noun', 'A3pl', 'P1pl'], formatted='[olmak:Verb] ol:Verb|abil:Able→Verb|dik:PastPart→Noun+ler:A3pl+imiz:P1pl')]\n",
      "------------------------------\n",
      "Word: okumuş\n",
      "Analysis: [Parse(word='okumuş', lemma='okumak', pos='Adj', morphemes=['Verb', 'NarrPart', 'Adj'], formatted='[okumak:Verb] oku:Verb|muş:NarrPart→Adj'), Parse(word='okumuş', lemma='okumak', pos='Verb', morphemes=['Verb', 'Narr', 'A3sg'], formatted='[okumak:Verb] oku:Verb+muş:Narr+A3sg')]\n",
      "------------------------------\n",
      "Word: gülmek\n",
      "Analysis: [Parse(word='gülmek', lemma='gülmek', pos='Noun', morphemes=['Verb', 'Inf1', 'Noun', 'A3sg'], formatted='[gülmek:Verb] gül:Verb|mek:Inf1→Noun+A3sg')]\n",
      "------------------------------\n",
      "Word: olta\n",
      "Analysis: [Parse(word='olta', lemma='olta', pos='Noun', morphemes=['Noun', 'A3sg'], formatted='[olta:Noun] olta:Noun+A3sg')]\n",
      "------------------------------\n",
      "Word: gelmektdir\n",
      "Analysis: [Parse(word='gelmektdir', lemma='Unk', pos='Unk', morphemes='Unk', formatted='Unk')]\n",
      "Analysis: []\n",
      "------------------------------\n",
      "Word: ağzı\n",
      "Analysis: [Parse(word='ağzı', lemma='ağız', pos='Noun', morphemes=['Noun', 'A3sg', 'Acc'], formatted='[ağız:Noun] ağz:Noun+A3sg+ı:Acc'), Parse(word='ağzı', lemma='ağız', pos='Noun', morphemes=['Noun', 'A3sg', 'P3sg'], formatted='[ağız:Noun] ağz:Noun+A3sg+ı:P3sg')]\n",
      "------------------------------\n",
      "Word: haddi\n",
      "Analysis: [Parse(word='haddi', lemma='had', pos='Noun', morphemes=['Noun', 'A3sg', 'Acc'], formatted='[had:Noun] hadd:Noun+A3sg+i:Acc'), Parse(word='haddi', lemma='had', pos='Noun', morphemes=['Noun', 'A3sg', 'P3sg'], formatted='[had:Noun] hadd:Noun+A3sg+i:P3sg')]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import zeyrek\n",
    "\n",
    "# Initialize the Morphological Analyzer\n",
    "analyzer = zeyrek.MorphAnalyzer()\n",
    "\n",
    "# Example Turkish words\n",
    "words = [\"gelmektedir\", \"yazmışlar\", \"olabildiklerimiz\",\n",
    "\"okumuş\", \"gülmek\", \"olta\", \"gelmektdir\", \"ağzı\", \"haddi\"]\n",
    "\n",
    "# Perform morphological analysis\n",
    "for word in words:\n",
    "    analysis = analyzer.analyze(word)\n",
    "    print(f\"Word: {word}\")\n",
    "    for item in analysis:\n",
    "        print(f\"Analysis: {item}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String similarity\n",
    "\n",
    "Edit distance using NLTK. Note that there might be faster libraries for high demanding tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit Distance between 'söylüyorum' and 'söyüyolrum': 2\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "string1 = \"söylüyorum\"\n",
    "string2 = \"söyüyolrum\"\n",
    "\n",
    "# Calculate Edit Distance\n",
    "distance = nltk.edit_distance(string1, string2)\n",
    "\n",
    "print(f\"Edit Distance between '{string1}' and '{string2}': {distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit Distance between 'söylüyorum' and 'söyüyolrum': 2\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "string1 = \"söylüyorum\"\n",
    "string2 = \"söyüyolrum\"\n",
    "\n",
    "# Calculate Levenshtein Distance\n",
    "distance = Levenshtein.distance(string1, string2)\n",
    "\n",
    "print(f\"Edit Distance between '{string1}' and '{string2}': {distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCS between 'söylüyorum' and 'söyüyolrum': 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14, 14, 16]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pylcs\n",
    "\n",
    "#  finding the longest common subsequence length of string A and string B\n",
    "string1 = \"söylüyorum\"\n",
    "string2 = \"söyüyolrum\"\n",
    "length = pylcs.lcs_sequence_length(string1, string2)\n",
    "print(f\"LCS between '{string1}' and '{string2}': {length}\")\n",
    "\n",
    "\n",
    "\n",
    "#  finding the longest common subsequence length of string A and a list of string B\n",
    "A = 'We are shannonai'\n",
    "B = ['We like shannonai', 'We work in shannonai', 'We are not shannonai']\n",
    "pylcs.lcs_sequence_of_list(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Prediction from Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bdaae7b49140ad91834a3510fe71c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: [CLS] I want to eat a hot [MASK] for lunch. [SEP]\n",
      "Predicted token: dog\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary) and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# Prepare the text\n",
    "text = \"[CLS] I want to eat a hot [MASK] for lunch. [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# Find the predicted token (we focus on the masked token)\n",
    "masked_index = tokenized_text.index(\"[MASK]\")\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Predicted token: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: [CLS] bugün hava çok [MASK]. [SEP]\n",
      "Predicted token: hava\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary) and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n",
    "model.eval()\n",
    "\n",
    "# Example sentence in Turkish with a masked word\n",
    "text = \"[CLS] bugün hava çok [MASK]. [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# Find the predicted token\n",
    "masked_index = tokenized_text.index(\"[MASK]\")\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Predicted token: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: [CLS] bugün hava çok [MASK]. [SEP]\n",
      "Predicted token: 全\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary) and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\")\n",
    "model = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Example sentence in Turkish with a masked word\n",
    "text = \"[CLS] bugün hava çok [MASK]. [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# Find the predicted token\n",
    "masked_index = tokenized_text.index(\"[MASK]\")\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Predicted token: {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text: ['word', '##pie', '##ce', 'token', '##ization', 'is', 'aw', '##es', '##ome', '!']\n",
      "Tokenized text: ['bugun', 'gunlerde', '##n', 'cumartesi', 've', 'cok', 'calıs', '##mam', 'lazım', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\")\n",
    "\n",
    "# Sample text\n",
    "text = \"WordPiece tokenization is awesome!\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Tokenized text:\", tokenized_text)\n",
    "\n",
    "text = \"Bugün günlerden cumartesi ve çok çalışmam lazım!\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Tokenized text:\", tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Bugün günlerden cumartesi', 'çalışmam lazım']\n",
      "Verbs: []\n"
     ]
    }
   ],
   "source": [
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"Bugün günlerden cumartesi ve çok çalışmam lazım!\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
