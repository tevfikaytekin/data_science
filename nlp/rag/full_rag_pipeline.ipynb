{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGd6Q8H_dOg0"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq PyPDF2\n",
        "!pip install -qqq chromadb\n",
        "!pip install -qqq sentence_transformers\n",
        "!pip install -qqq langchain\n",
        "!pip install -qqq groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "w56v5qN7jxb_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import chromadb\n",
        "from google.colab import files  # For Colab file uploading\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import textwrap\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "# Groq API key\n",
        "API_KEY = \"GROQ_API_KEY\"\n",
        "\n",
        "# Initialize the Groq client\n",
        "client = Groq(api_key=API_KEY)\n",
        "\n",
        "# Set model and parameters for Groq API\n",
        "MODEL = \"llama-3.1-70b-versatile\"\n",
        "TEMPERATURE = 0.7\n",
        "MAX_TOKENS = 500  # Adjust based on your needs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hf9nf96mjqnm"
      },
      "outputs": [],
      "source": [
        "# Step 1: Extract text from PDF files\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        pdf_reader = PyPDF2.PdfReader(f)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Step 2: Load PDFs from a specified directory in Colab\n",
        "def load_pdfs(directory_path):\n",
        "    all_texts = []\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(directory_path, filename)\n",
        "            text = extract_text_from_pdf(pdf_path)\n",
        "            all_texts.append(text)\n",
        "    return all_texts\n",
        "\n",
        "# Step 3: Use Recursive Text Splitter to split the text into chunks\n",
        "def split_text_recursive(text, chunk_size=500, chunk_overlap=50):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    chunks = splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "# Step 4: Embed text chunks using a pre-trained transformer model\n",
        "def embed_texts(chunks):\n",
        "    model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"  # Replace with a Turkish-specific model if needed.\n",
        "    embedding_model = SentenceTransformer(model_name)\n",
        "    embeddings = embedding_model.encode(chunks, show_progress_bar=True)\n",
        "    return embeddings\n",
        "\n",
        "# Step 5: Store embeddings in Chroma\n",
        "def create_chroma_db(chunks, embeddings):\n",
        "    collection_name = \"document_chunks\"  # Choose a name for your collection\n",
        "    client = chromadb.Client()  # Initialize Chroma client\n",
        "\n",
        "    try:\n",
        "        # Try to delete the collection if it already exists\n",
        "        client.delete_collection(collection_name)\n",
        "        print(f\"Deleted existing collection: {collection_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not delete collection: {e}\")\n",
        "\n",
        "    collection = client.create_collection(collection_name)\n",
        "\n",
        "    # Add chunks and their corresponding embeddings to the Chroma collection\n",
        "    collection.add(\n",
        "        documents=chunks,\n",
        "        embeddings=embeddings.tolist(),\n",
        "        metadatas=[{\"chunk\": i} for i in range(len(chunks))],  # Metadata to track each chunk\n",
        "        ids=[str(i) for i in range(len(chunks))]\n",
        "    )\n",
        "    return collection\n",
        "\n",
        "# Step 6: Retrieve relevant chunks from Chroma based on the user's question\n",
        "def retrieve_relevant_chunks(question, embedding_model, collection, top_k=3):\n",
        "    question_embedding = embedding_model.encode([question])[0]  # Embed the user's question\n",
        "\n",
        "    # Perform a search in Chroma and retrieve the top_k most similar chunks\n",
        "    results = collection.query(query_embeddings=[question_embedding.tolist()], n_results=top_k)\n",
        "\n",
        "    # Extract the retrieved chunks\n",
        "    retrieved_chunks = [doc for doc in results['documents'][0]]\n",
        "    return retrieved_chunks\n",
        "\n",
        "# Step 7: Use Groq API via the GroqClient to generate answers using chat completions\n",
        "def generate_answer_with_groq(question, retrieved_chunks):\n",
        "    context = \"\\n\".join(retrieved_chunks)  # Combine retrieved chunks into context\n",
        "\n",
        "    # Format messages in a chat-like structure for the Groq API\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an assistant for answering questions based on the provided context.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Context: {context}\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
        "    ]\n",
        "\n",
        "    # Use the Groq client to send the messages for inference\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        temperature=TEMPERATURE,\n",
        "        max_tokens=MAX_TOKENS,\n",
        "    )\n",
        "\n",
        "    # Parse the response from the Groq API\n",
        "    if response:\n",
        "        answer = response.choices[0].message\n",
        "        #answer = response.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No answer generated.\")\n",
        "        return answer\n",
        "    else:\n",
        "        return \"Could not generate an answer.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o8EvJelkkNm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Upload PDF files in Colab manually or mount Google Drive\n",
        "print(\"Please upload PDF files.\")\n",
        "uploaded_files = files.upload()  # Use the Colab file uploader\n",
        "\n",
        "# Save uploaded files to a temporary folder\n",
        "directory_path = \"/content/pdf_files\"\n",
        "os.makedirs(directory_path, exist_ok=True)\n",
        "\n",
        "for filename in uploaded_files.keys():\n",
        "    with open(os.path.join(directory_path, filename), \"wb\") as f:\n",
        "        f.write(uploaded_files[filename])\n",
        "\n",
        "# Step 2: Load and extract text from PDFs\n",
        "texts = load_pdfs(directory_path)\n",
        "\n",
        "# Step 3: Split the text into smaller chunks using the recursive splitter\n",
        "text_chunks = []\n",
        "for text in texts:\n",
        "    chunks = split_text_recursive(text)\n",
        "    text_chunks.extend(chunks)\n",
        "\n",
        "# Step 4: Embed the text chunks\n",
        "embeddings = embed_texts(text_chunks)\n",
        "\n",
        "# Step 5: Create a Chroma DB and store embeddings\n",
        "collection = create_chroma_db(text_chunks, embeddings)\n",
        "\n",
        "# Step 6: Ask questions\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")  # Same model as used for embedding text\n",
        "\n",
        "while True:\n",
        "    question = input(\"\\nEnter your question: \")\n",
        "    if question.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Retrieve relevant chunks based on the user's question\n",
        "    retrieved_chunks = retrieve_relevant_chunks(question, embedding_model, collection)\n",
        "\n",
        "    # Generate an answer based on the retrieved chunks and the question using Groq API\n",
        "    answer = generate_answer_with_groq(question, retrieved_chunks)\n",
        "    wrapped_lines = [textwrap.fill(line, width=80) for line in answer.content.splitlines()]\n",
        "    wrapped_text = \"\\n\".join(wrapped_lines)\n",
        "    print(\"\\n\")\n",
        "    print(wrapped_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8sEAuGJgedS",
        "outputId": "dbb7be00-a2a6-48ff-bca5-173d8c6b6062"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "132"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "len(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLL1UNB5hCEc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}